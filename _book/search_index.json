[["index.html", "Spatial Data Science with applications in R Preface Acknowledgements", " Spatial Data Science with applications in R Edzer Pebesma, Roger Bivand 2022-05-18 Preface Data science is concerned with finding answers to questions on the basis of available data, and communicating that effort. Besides showing the results, this communication involves sharing the data used, but also exposing the path that led to the answers in a comprehensive and reproducible way. It also acknowledges the fact that available data may not be sufficient to answer questions, and that any answers are conditional on the data collection or sampling protocols employed. This book introduces and explains the concepts underlying spatial data: points, lines, polygons, rasters, coverages, geometry attributes, data cubes, reference systems, as well as higher-level concepts including how attributes relate to geometries and how this affects analysis. The relationship of attributes to geometries is known as support, and changing support also changes the characteristics of attributes. Some data generation processes are continuous in space, and may be observed everywhere. Others are discrete, observed in tesselated containers. In modern spatial data analysis, tesellated methods are often used for all data, extending across the legacy partition into point process, geostatistical and lattice models. It is support (and the understanding of support) that underlies the importance of spatial representation. The book aims at data scientists who want to get a grip on using spatial data in their analysis. To exemplify how to do things, it uses R. It is often thought that spatial data boils down to having observations’ longitude and latitude in a dataset, and treating these just like any other variable. This carries the risk of missed opportunities and meaningless analyses. For instance, coordinate pairs really are pairs, and lose much of their meaning when treated independently rather than having point locations, observations are often associated with spatial lines, areas, or grid cells spatial distances between observations are often not well represented by straight-line distances, but by great circle distances, distances through networks, or by measuring the effort it takes getting from A to B We introduce the concepts behind spatial data, coordinate reference systems, spatial analysis, and introduce a number of packages, including sf (Pebesma 2018, 2022a), stars (Pebesma 2022b), s2 (Dunnington, Pebesma, and Rubak 2022) and lwgeom (Pebesma 2021), as well as a number of tidyverse (Wickham 2021) extensions, and a number of spatial analysis and visualisation packages that can be used with these packages, including gstat (Pebesma and Graeler 2022), spdep (Bivand 2022), spatialreg (Bivand and Piras 2022), spatstat (Baddeley, Turner, and Rubak 2022), tmap (Tennekes 2022) and mapview (Appelhans et al. 2022). Acknowledgements all GitHub contributors (t.b.d.), Sahil Bhandari, Claus Wilke, Jakub Nowosad, SDSWR class summer 2021, all sf and stars authors, This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. References "],["intro.html", "Chapter 1 Getting Started 1.1 A first map 1.2 Coordinate reference systems 1.3 Raster and vector data 1.4 Raster types 1.5 Time series, arrays, data cubes 1.6 Support 1.7 Spatial data science software 1.8 Exercises", " Chapter 1 Getting Started This chapter introduces a number of concepts associated with handling spatial data, and points forward to later sections where they are discussed in more detail. The code sections in this chapter are not explained, but can be unfolded by clicking on the “code” button. They should be easy to follow when understanding R at the level of, say, R for Data Science (Wickham and Grolemund 2017). Detailed explanation of R code starts in Part II of this book. 1.1 A first map The typical way to graph spatial data is by creating a map. Let us consider a simple map, shown in figure 1.1. library(tidyverse) library(sf) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) |&gt; read_sf() -&gt; nc nc.32119 &lt;- st_transform(nc, &#39;EPSG:32119&#39;) nc.32119 |&gt; select(BIR74) |&gt; plot(graticule = TRUE, axes = TRUE) Figure 1.1: a first map A number of graphical elements are present here, in this case: polygons are drawn with a black outline and filled with colors chosen according to a variable BIR74, whose name is in the title a legend key explains the meaning of the colors, and has a certain color palette and color breaks, values at which color changes the background of the map shows curved lines with constant latitude or longitude (graticule) the axis ticks show the latitude and longitude values Polygons are a particular form of geometry; spatial geometries (points, lines, polygons, pixels) are discussed in detail in chapter 3. Polygons consist of sequences of points, connected by straight lines. How point locations of spatial data are expressed, or measured, is discussed in chapter 2. As can be seen from figure 1.1, lines of equal latitude and longitude do not form straight lines, indicating that some form of projection took place before plotting; projections are also discussed in chapter 2 and section ??. The color values in figure 1.1 are derived from numeric values of a variable, BIR74, which has a single value associated with each geometry or feature. Chapter ?? discusses such feature attributes, and the way they can relate to feature geometries. In this case, BIR74 refers to birth counts, meaning counts over the region. This implies that the count does not refer to a value associated with every point inside the polygon, which the continuous color might suggest, but rather measures an integral (sum) over the polygon. Before plotting figure 1.1 we had to read the data, in this case from a file (section ??). Printing a data summary for the first three records of three attribute variables shows: nc |&gt; select(AREA, BIR74, SID74) |&gt; print(n = 3) # Simple feature collection with 100 features and 3 fields # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6 # Geodetic CRS: NAD27 # # A tibble: 100 × 4 # AREA BIR74 SID74 geom # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; # 1 0.114 1091 1 (((-81.5 36.2, -81.5 36.3, -81.6 36.3, -81.6 36.3, -81.7 36… # 2 0.061 487 0 (((-81.2 36.4, -81.2 36.4, -81.3 36.4, -81.3 36.4, -81.3 36… # 3 0.143 3188 5 (((-80.5 36.2, -80.5 36.3, -80.5 36.3, -80.5 36.3, -80.6 36… # # … with 97 more rows The printed output shows: the (selected) dataset has 100 features (records) and 3 fields (attributes) the geometry type is MULTIPOLYGON (chapter 3) it has dimension XY, indicating that each point will consist of 2 coordinate values the range of x and y values of the geometry the coordinate reference system (CRS) is geodetic, with coordinates in degrees longitude and latitude associated to the NAD27 datum (chapter 2) the three selected attribute variables are followed by a variable geom of type MULTIPOLYGON with unit degrees that contains the polygon information More complicated plots can involve facet plots with a map in each facet, as shown in figure 1.2. year_labels = c(&quot;SID74&quot; = &quot;1974 - 1978&quot;, &quot;SID79&quot; = &quot;1979 - 1984&quot;) nc.32119 |&gt; select(SID74, SID79) |&gt; pivot_longer(starts_with(&quot;SID&quot;)) -&gt; nc_longer ggplot() + geom_sf(data = nc_longer, aes(fill = value)) + facet_wrap(~ name, ncol = 1, labeller = labeller(name = year_labels)) + scale_y_continuous(breaks = 34:36) + scale_fill_gradientn(colors = sf.colors(20)) + theme(panel.grid.major = element_line(color = &quot;white&quot;)) Figure 1.2: ggplot with facet maps An interactive, leaflet-based map is obtained in figure ??. suppressPackageStartupMessages(library(mapview)) mapviewOptions(fgb = FALSE) nc.32119 |&gt; mapview(zcol = &quot;BIR74&quot;, legend = TRUE, col.regions = sf.colors) 1.2 Coordinate reference systems In figure 1.1, the grey lines denote the graticule, a grid with lines along constant latitude or longitude. Clearly, these lines are not straight, which indicates that a projection of the data was used for which the x and y axis do not align with longitude and latitude. In figure ?? we see that the north boundary of North Carolina is plotted as a straight line again, indicating that another projection was used. The ellipsoidal coordinates of the graticule of figure 1.1 are associated with a particular datum (here: NAD27), which implicates a set of rules what the shape of the Earth is and how it is attached to the Earth (to which point of the Earth is the origin associated, and how is it directed). If one would measure coordinates with a GPS device (e.g. a mobile phone) it would typically report coordinates associated with the WGS84 datum, which can be around 30 m different from the identical coordinate values when associated with NAD27. Projections describe how we go back and forth between ellipsoidal coordinates which are expressed as degrees latitude and longitude, pointing to locations on a shape approximating the Earth’s shape (an ellipsoid or spheroid), and projected coordinates which are coordinates on a flat, two-dimensional coordinate system, used when plotting maps. Datums transformations are associated with moving from one datum to another. Both topics are covered by spatial reference systems are described in more detail in chapter 2. 1.3 Raster and vector data Polygon, point and line geometries are examples of vector data: point coordinates describe the “exact” locations that can be anywhere. Raster data on the other hand describe data where values are aligned on a raster, meaning on a regularly laid out lattice of usually square pixels. An example is shown in figure 1.3. library(stars) par(mfrow = c(2, 2)) par(mar = rep(1, 4)) tif &lt;- system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) x &lt;- read_stars(tif)[,,,1] image(x, main = &quot;(a)&quot;) image(x[,1:10,1:10], text_values = TRUE, border = &#39;grey&#39;, main = &quot;(b)&quot;) image(x, main = &quot;(c)&quot;) set.seed(131) pts = st_sample(st_as_sfc(st_bbox(x)), 3) plot(pts, add = TRUE, pch = 3, col = &#39;blue&#39;) image(x, main = &quot;(d)&quot;) plot(st_buffer(pts, 500), add = TRUE, pch = 3, border = &#39;blue&#39;, col = NA, lwd = 2) Figure 1.3: raster maps: Landsat-7 blue band, with color values derived from data values (a), the top-left 10x10 sub-image from (a) with numeric values shown (b), and overlayed by two different types of vector data: three sample points (c), and a 500m radius around the points represented as polygons (d) Vector and raster data can be combined in different ways; for instance we can query the raster at the three points of figure 1.3(c), st_extract(x, pts) # Simple feature collection with 3 features and 1 field # Geometry type: POINT # Dimension: XY # Bounding box: xmin: 290000 ymin: 9110000 xmax: 292000 ymax: 9120000 # Projected CRS: SIRGAS 2000 / UTM zone 25S # L7_ETMs.tif geometry # 1 80 POINT (290830 9114499) # 2 58 POINT (290019 9119219) # 3 63 POINT (291693 9116038) or compute an aggregate, such as the average, over arbitrary regions such as the circles shown in figure 1.3(d): aggregate(x, st_buffer(pts, 500), FUN = mean) |&gt; st_as_sf() # Simple feature collection with 3 features and 1 field # Geometry type: POLYGON # Dimension: XY # Bounding box: xmin: 290000 ymin: 9110000 xmax: 292000 ymax: 9120000 # Projected CRS: SIRGAS 2000 / UTM zone 25S # V1 geometry # 1 77.2 POLYGON ((291330 9114499, 2... # 2 60.1 POLYGON ((290519 9119219, 2... # 3 71.6 POLYGON ((292193 9116038, 2... Other raster-to-vector conversions are discussed in ?? and include: converting raster pixels into point values converting raster pixels into small polygons, possibly merging polygons with identical values (“polygonize”) generating lines or polygons that delineate continuous pixel areas with a certain value range (“contour”) plot(st_rasterize(nc[&quot;BIR74&quot;], dx = 0.1), col = sf.colors(), breaks = &quot;equal&quot;) Figure 1.4: map obtained by rasterizing county total number of births for the period 1974-1979 shown in figure 1.1 Vector-to-raster conversions can be as simple as rasterizing polygons, as shown in figure 1.4. Other, more general vector-to-raster conversions that may involve statistical modelling include: interpolation of point values to points on a regular grid (chapter ??) estimating densities of points over a regular grid (chapter ??) area-weighted interpolation of polygon values to grid cells (section ??) direct rasterization of points, lines or polygons (section ??) 1.4 Raster types Raster dimensions describe how the rows and columns relate to spatial coordinates. Figure 1.5 shows a number of different possibilities. x = 1:5 y = 1:4 d = st_dimensions(x = x, y = y, .raster = c(&quot;x&quot;, &quot;y&quot;)) m = matrix(runif(20),5,4) r1 = st_as_stars(r = m, dimensions = d) r = attr(d, &quot;raster&quot;) r$affine = c(0.2, -0.2) attr(d, &quot;raster&quot;) = r r2 = st_as_stars(r = m, dimensions = d) r = attr(d, &quot;raster&quot;) r$affine = c(0.1, -0.3) attr(d, &quot;raster&quot;) = r r3 = st_as_stars(r = m, dimensions = d) x = c(1, 2, 3.5, 5, 6) y = c(1, 1.5, 3, 3.5) d = st_dimensions(x = x, y = y, .raster = c(&quot;x&quot;, &quot;y&quot;)) r4 = st_as_stars(r = m, dimensions = d) grd = st_make_grid(cellsize = c(10,10), offset = c(-130,10), n = c(8,5), crs = st_crs(4326)) r5 = st_transform(grd, &quot;+proj=laea +lon_0=-70 +lat_0=35&quot;) par(mfrow = c(2,3), mar = c(0.1, 1, 1.1, 1)) r1 = st_make_grid(cellsize = c(1,1), n = c(5,4), offset = c(0,0)) plot(r1, main = &quot;regular&quot;) plot(st_geometry(st_as_sf(r2)), main = &quot;rotated&quot;) plot(st_geometry(st_as_sf(r3)), main = &quot;sheared&quot;) plot(st_geometry(st_as_sf(r4, as_points = FALSE)), main = &quot;rectilinear&quot;) plot(st_geometry((r5)), main = &quot;curvilinear&quot;) Figure 1.5: various raster types Regular rasters like shown in figure 1.5 have a constant, not necessarily square cell size and axes aligned with the x and y (Easting and Northing) axes. Other raster types include those where the axes are no longer aligned with x and y (rotated), where axes are no longer perpendicular (sheared), or where cell size varies along a dimension (rectilinear). Finally, curvilinear rasters have cell size and/or direction properties that are no longer independent from the other raster dimension. When a raster that is regular in a given coordinate reference system is projected to another raster while keeping each raster cell in tact, it changes shape and may become rectilinear (e.g. when going from ellipsoidal coordinates to Mercator, as in figure ??) or curvilinear (e.g. when going from ellipsoidal coordinates to Lambert Conic Conformal, as in figure 1.1). When reverting this procedure, one can recover the exact original raster. Creating a new, regular grid in the new projection is called raster (or image) reprojection or warping (section ??). This process is lossy, irreversible, and may need to be informed whether raster cells should be interpolated, averaged or summed, whether they denote categorical variables, or whether resampling using nearest neighbours should be used; see also section 1.6. 1.5 Time series, arrays, data cubes A lot of spatial data is not just spatial, but in addition temporal. Just like any observation is associated with an observation location, it is associated with an observation time or period. The dataset on the North Carolina counties shown above contains disease cases counted over two time periods, shown in figure 1.2. Although the original dataset has these variables in two different columns, for plotting them these columns had to be stacked first, while repeating the associated geometries - a form called tidy by (Wickham 2014). When we have longer time series associated with geometries, neither option - distributing time over multiple columns, or stacking columns while repeating geometries - works well, and a more effective way of storing such data would be a matrix or array, where one dimension refers to time, and the other(s) to space. The natural way for image or raster data is already to store them in matrices; time series of rasters then lead to a three-dimensional array. The general term for such data is a (spatiotemporal) data cube, where cube refers to arrays with any number of dimensions. Data cubes can refer to both raster and vector data, examples are given in chapter ??. 1.6 Support When we have spatial data with geometries that are not points but collections of points (multi-points, lines, polygons, pixels), then the attributes associated with these geometries has one of several different relationships to them. Attributes can have: a constant value for every point of the geometry a value that is unique to only this geometry, describing its identity a single value that is an aggregate over all points of the geometry An example of a constant is land use or bedrock type of a polygon. An example of an identity is a county name. An example of an aggregate is the number of births over a given period of time, of a county. The area with to which an attribute value refers to is called its support: aggregate properties have “block” (or polygon, or line) support, constant properties have “point” support (they apply to every point). Support matters when we manipulate the data. For instance, figure 1.4 was derived from a variable that has polygon support: the number of births per county. Rasterizing these values gives pixels with values that are associated to counties. The result of the rasterization is a meaningless map: the numeric values (“birth totals”) are not associated with the raster cells, and the county boundaries are no longer present. Totals of birth for the whole state can no longer be recovered from the pixel values. Ignoring support can easily lead to meaningless results. Chapter ?? discusses this further. Raster cell values may have point support, e.g. when the cell records the elevation of the point at the cell centre in a digital elevation model, or cell support, e.g. when a satellite image pixel gives the color value averaged over (an area similar to the) pixel. Most file formats do not provide this information, yet it may be important to know when aggregating, regridding or warping rasters (section ??). 1.7 Spatial data science software Although this book largely uses R and R packages for spatial data science, a number of these packages use software libraries that were not developed for R specifically. As an example, the dependency of R package sf on other R packages and system libraries is shown in figure 1.6. Figure 1.6: sf and its dependencies; arrows indicate strong dependency, dashed arrows weak dependency The C or C++ libraries used (GDAL, GEOS, PROJ, liblwgeom, s2geometry, NetCDF, udunits2) are all developed, maintained and used by (spatial) data science communities that are large and mostly different from the R community. By using these libraries, R users share how we understand what we are doing with these other communities. Because R, Python and Julia provide interactive interfaces to this software, many users get closer to these libraries than do users of other software based on these libraries. The first part of this book describes many of the concepts implemented in these libraries, which is relevant to spatial data science in general. 1.7.1 GDAL GDAL (“Geospatial Data Abstraction Library”) can be seen as the Swiss army knife of spatial data; besides for R it is being used in Python, QGIS, PostGIS, and more than 100 other software projects. GDAL is a “library of libraries” – in order to read all these data sources it needs a large number of other libraries. It typically links to over 100 other libraries, each of which provides access to e.g. a particular data file format, database access, or web service. Binary R packages distributed by CRAN contain only statically linked code: CRAN does not want to make any assumptions about presence of third-party libraries on the host system. As a consequence, when the sf package is installed in binary form from CRAN, it includes a copy of all the required external libraries as well as their dependencies, which may amount to 100 Mb. 1.7.2 PROJ PROJ (or PR\\(\\phi\\)J) is a library for cartographic projections and datum transformations: it converts spatial coordinates from one coordinate reference system to another. It comes with a large database of known projections and access to datum grids (high-precision pre-calculated values for datum transformations). It aligns with an international standard for coordinate reference systems (Lott 2015). Chapter 2 deals with coordinate systems, and PROJ. 1.7.3 GEOS and s2geometry GEOS (“Geometry Engine Open Source”) and s2geometry are two libraries for geometric operations. They are used to find measures (length, area, distance), and calculate predicates (do two geometries have any points in common?) or new geometries (which points do these two geometries have in common?). GEOS does this for flat, two-dimensional space (indicated by \\(R^2\\)), s2geometry does this for geometries on the sphere (indicated by \\(S^2\\)). Chapter 2 introduces coordinate reference systems, and chapter ?? discusses more about the differences between working with these two spaces. 1.7.4 NetCDF, udunits2, liblwgeom NetCDF (UCAR 2020) refers to a file format as well as a C library for reading and writing NetCDF files. It allows the definition of arrays of any dimensionality, and is widely used for spatial and spatiotemporal information, especially in the (climate) modelling communities. Udunits2 (UCAR 2014; Pebesma et al. 2022) is a database and software library for units of measurement that allows the conversion of units, handles derived units, and supports user-defined units. The liblwgeom “library” is a software component of PostGIS (Obe and Hsu 2015) that contains several routines missing from GDAL or GEOS, including convenient access to GeographicLib routines (Karney 2013) that ship with PROJ. 1.8 Exercises List five differences between raster and vector data. In addition to those listed below figure 1.1, list five further graphical components that are often found on a map. In your own words, why is the numeric information shown in figure 1.4 misleading (or meaningless)? Under which conditions would you expect strong differences when doing geometrical operations on \\(S^2\\), compared to doing them on \\(R^2\\)? References "],["cs.html", "Chapter 2 Coordinates 2.1 Quantities, units, datum 2.2 Ellipsoidal coordinates 2.3 Coordinate Reference Systems 2.4 PROJ and mapping accuracy 2.5 WKT-2 2.6 Exercises", " Chapter 2 Coordinates “Data are not just numbers, they are numbers with a context”; “In data analysis, context provides meaning” (Cobb and Moore 1997) Before we can try to understand geometries like points, lines, polygons, coverage and grids, it is useful to review coordinate systems so that we have an idea what exactly coordinates of a point reflect. For spatial data, the location of observations are characterized by coordinates, and coordinates are defined in a coordinate system. Different coordinate systems can be used for this, and the most important difference is whether coordinates are defined over a 2-dimensional or 3-dimensional space referenced to orthogonal axes (Cartesian coordinates), or using distance and directions (polar coordinates, spherical and ellipsoidal coordinates). Besides a location of observation, all observations are associated with time of observation, and so time coordinate systems are also briefly discussed. First we will briefly review quantities, to learn what units and datum are. 2.1 Quantities, units, datum The VIM (“International Vocabulary of Metrology”, BIPM et al. (2012)) defines a quantity as a “property of a phenomenon, body, or substance, where the property has a magnitude that can be expressed as a number and a reference”, where “[a] reference can be a measurement unit, a measurement procedure, a reference material, or a combination of such.” Although one could argue whether all data is constituted of quantities, there is no need to argue that proper data handling requires that numbers (or symbols) are accompanied by information on what they mean, in particular what they refer to. A measurement system consist of base units for base quantities, and derived units for derived quantities. For instance, the SI system of units (Bureau International des Poids et Mesures 2006) consist of the seven base units length (metre, m), mass (kilogram, kg), time (second, s), electric current (ampere, A), thermodynamic temperature (Kelvin, K), amount of substance (mole, mol), and luminous intensity (candela, cd). Derived units are composed of products of integer powers of base units; examples are speed (\\(\\mbox{m}~\\mbox{s}^{-1}\\)), density (\\(\\mbox{kg}~\\mbox{m}^{-3}\\)) and area (\\(\\mbox{m}^2\\)). The special case of unitless measures can refer to either cases where units cancel out (e.g. mass fraction: kg/kg, or angle measured in rad: m/m) or to cases where objects or events were counted (e.g. 5 apples). Adding an angle to a count of apples would not make sense; adding 5 apples to 3 oranges may make sense if the result is reinterpreted as a superclass, e.g. as pieces of fruit. Many data variables have units that are not expressible as SI base units or derived units. Hand (2004) discusses many such measurement scales, e.g. those used to measure intelligence in social sciences, in the context of measurement units. For many quantities, the natural origin of values is zero. This works for amounts, where differences between amounts result in meaningful negative values. For locations and times, differences have a natural zero interpretation: distance and duration. Absolute location (position) and time need a fixed origin, from which we can meaningfully measure other absolute space-time points: we call this a datum. For space, a datum involves more than one dimension. The combination of a datum and a measurement unit (scale) is a reference system. We will now elaborate how spatial locations can be expressed as either ellipsoidal or Cartesian coordinates. The next sections will deal with temporal and spatial reference systems, and how they are handled in R. 2.2 Ellipsoidal coordinates par(mar = rep(0,4)) plot(3, 4, xlim = c(-6,6), ylim = c(-6,6), asp = 1) axis(1, pos = 0, at = 0:6) axis(2, pos = 0, at = -6:6) xd = seq(-5, 5, by = .1) lines(xd, sqrt(25 - xd^2), col = &#39;grey&#39;) lines(xd, -sqrt(25 - xd^2), col = &#39;grey&#39;) arrows(0, 0, 3, 4, col = &#39;red&#39;, length = .15, angle = 20) text(1.5, 2.7, label = &quot;r&quot;, col = &#39;red&#39;) xd = seq(3/5, 1, by = .1) lines(xd, sqrt(1 - xd^2), col = &#39;red&#39;) text(1.2, 0.5, label = parse(text = &quot;phi&quot;), col = &#39;red&#39;) lines(c(3,3), c(0,4), lty = 2, col = &#39;blue&#39;) lines(c(0,3), c(4,4), lty = 2, col = &#39;blue&#39;) text(3.3, 0.3, label = &quot;x&quot;, col = &#39;blue&#39;) text(0.3, 4.3, label = &quot;y&quot;, col = &#39;blue&#39;) Figure 2.1: Two-dimensional polar (red) and Cartesian (blue) coordinates Figure 2.1 shows both polar and Cartesian coordinates for a two-dimensional situation. In Cartesian coordinates, the point shown is \\((x,y) = (3,4)\\), for polar coordinates it is \\((r,\\phi) = (5, \\mbox{arctan}(4/3))\\), where \\(\\mbox{arctan}(4/3)\\) is approximately \\(0.93\\) radians, or \\(53^{\\circ}\\). Note that \\(x\\), \\(y\\) and \\(r\\) all have length units, where \\(\\phi\\) is an angle (a unitless length/length ratio). Converting back and forth between Cartesian and polar coordinates is trivial, as \\[x = r~\\mbox{cos} \\phi,\\] \\[y = r~\\mbox{sin} \\phi,\\] \\[r = \\sqrt{x^2 + y^2}, \\ \\mbox{and}\\] \\[\\phi = \\mbox{atan2}(y, x)\\] where \\(\\mbox{atan2}\\) is used in favor of \\(\\mbox{atan}(y/x)\\) to take care of the right quadrant. 2.2.1 Ellipsoidal coordinates In three dimensions, where Cartesian coordinates are expressed as \\((x,y,z)\\), spherical coordinates are the three-dimensional equivalent of polar coordinates and can be expressed as \\((r,\\lambda,\\phi)\\), where: \\(r\\) is the radius of the sphere, \\(\\lambda\\) is the longitude, measured in the \\((x,y)\\) plane counter-clockwise from positive \\(x\\), and \\(\\phi\\) is the latitude, the angle between the vector and the \\((x,y)\\) plane. Figure 2.2 illustrates Cartesian geocentric and ellipsoidal coordinates. suppressPackageStartupMessages(library(sf)) e = cbind(-90:90,0) # equator f1 = rbind(cbind(0, -90:90)) # 0/antimerid. f2 = rbind(cbind(90, -90:90), cbind(270, 90:-90))# +/- 90 eq = st_sfc(st_linestring(e), st_linestring(f1), st_linestring(f2), crs=4326) geoc = st_transform(eq, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]], NA, geoc[[2]], NA, geoc[[3]]) from3d = function(x, offset, maxz, minz) { x = x[,c(2,3,1)] + offset # move to y right, x up, z backw x[,2] = x[,2] - maxz # shift y to left d = maxz z = x[,3] - minz + offset x[,1] = x[,1] * (d/z) x[,2] = x[,2] * (d/z) x[,1:2] } maxz = max(cc[,3], na.rm = TRUE) minz = min(cc[,3], na.rm = TRUE) offset = 3e7 circ = from3d(cc, offset, maxz, minz) mx = max(cc, na.rm = TRUE) * 1.1 x = rbind(c(0, 0, 0), c(mx, 0, 0)) y = rbind(c(0, 0, 0), c(0, mx, 0)) z = rbind(c(0, 0, 0), c(0, 0, mx)) ll = rbind(x, NA, y, NA, z) l0 = from3d(ll, offset, maxz, minz) mx = max(cc, na.rm = TRUE) * 1.2 x = rbind(c(0, 0, 0), c(mx, 0, 0)) y = rbind(c(0, 0, 0), c(0, mx, 0)) z = rbind(c(0, 0, 0), c(0, 0, mx)) ll = rbind(x, NA, y, NA, z) l = from3d(ll, offset, maxz, minz) par(mfrow = c(1, 2)) par(mar = rep(0,4)) plot.new() plot.window(xlim = c(min(circ[,1],na.rm = TRUE), 3607103*1.02), ylim = c(min(circ[,2],na.rm = TRUE), 2873898*1.1), asp = 1) lines(circ) lines(l0) text(l[c(2,5,8),], labels = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), col = &#39;red&#39;) # add POINT(60 47) p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) |&gt; st_transform(&quot;+proj=geocent&quot;) p = p[[1]] pts = rbind(c(0,0,0), c(p[1],0,0), c(p[1],p[2],0), c(p[1],p[2],p[2])) ptsl = from3d(pts, offset, maxz, minz) lines(ptsl, col = &#39;blue&#39;, lty = 2, lwd = 2) points(ptsl[4,1], ptsl[4,2], col = &#39;blue&#39;, cex = 1, pch = 16) plot.new() plot.window(xlim = c(min(circ[,1],na.rm = TRUE), 3607103*1.02), ylim = c(min(circ[,2],na.rm = TRUE), 2873898*1.1), asp = 1) lines(circ) p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) |&gt; st_transform(&quot;+proj=geocent&quot;) p = p[[1]] pts = rbind(c(0,0,0), c(p[1],p[2],p[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) points(pt[2,1], pt[2,2], col = &#39;blue&#39;, cex = 1, pch = 16) p0 = st_as_sfc(&quot;POINT(60 0)&quot;, crs = 4326) |&gt; st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) p0 = st_as_sfc(&quot;POINT(0 0)&quot;, crs = 4326) |&gt; st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt) p0 = st_as_sfc(&quot;POINT(0 90)&quot;, crs = 4326) |&gt; st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt, lty = 2) p0 = st_as_sfc(&quot;POINT(90 0)&quot;, crs = 4326) |&gt; st_transform(&quot;+proj=geocent&quot;) p0 = p0[[1]] pts = rbind(c(0,0,0), c(p0[1],p0[2],p0[3])) pt = from3d(pts, offset, maxz, minz) lines(pt, lty = 2) f1 = rbind(cbind(0:60, 0)) arc = st_sfc(st_linestring(f1), crs=4326) geoc = st_transform(arc, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]]) circ = from3d(cc, offset, maxz, minz) lines(circ, col = &#39;red&#39;, lwd = 2, lty = 2) f1 = rbind(cbind(60, 0:47)) arc = st_sfc(st_linestring(f1), crs=4326) geoc = st_transform(arc, &quot;+proj=geocent&quot;) cc = rbind(geoc[[1]]) circ = from3d(cc, offset, maxz, minz) lines(circ, col = &#39;blue&#39;, lwd = 2, lty = 2) text(pt[1,1]+100000, pt[1,2]+50000, labels = expression(phi), col = &#39;blue&#39;) # lat text(pt[1,1]+20000, pt[1,2]-50000, labels = expression(lambda), col = &#39;red&#39;) # lng Figure 2.2: Cartesian geocentric coordinates (left) measure three distances, ellipsoidal coordinates (right) measure two angles, and possibly an ellipsoidal height \\(\\lambda\\) typically varies between \\(-180^{\\circ}\\) and \\(180^{\\circ}\\) (or alternatively from \\(0^{\\circ}\\) to \\(360^{\\circ}\\)), \\(\\phi\\) from \\(-90^{\\circ}\\) to \\(90^{\\circ}\\). When we are only interested in points on a sphere with given radius, we can drop \\(r\\): \\((\\lambda,\\phi)\\) now suffice to identify any point. It should be noted that this is just a definition, one could for instance also choose to measure polar angle, the angle between the vector and \\(z\\), instead of latitude. There is also a long tradition of specifying points as \\((\\phi,\\lambda)\\) but throughout this book we will stick to longitude-latitude, \\((\\lambda,\\phi)\\). The point denoted in figure 2.2 has \\((\\lambda,\\phi)\\) or ellipsoidal coordinates with values p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) p[[1]] # POINT (60 47) with angles measured in degrees, and geocentric coordinates p = st_as_sfc(&quot;POINT(60 47)&quot;, crs = 4326) |&gt; st_transform(&quot;+proj=geocent&quot;) p[[1]] # POINT Z (2178844 3773868 4641765) with unit metres. For points on an ellipse, there are two ways in which angle can be expressed (figure 2.3): measured from the center of the ellipse (\\(\\psi\\)), or measured perpendicular to the tangent on the ellipse at the target point (\\(\\phi\\)). par(mar = rep(0,4)) x = 4 y = 5/8 * sqrt(48) plot(x, y, xlim = c(-6,6), ylim = c(-8,8), asp = 1) axis(1, pos = 0, at = 0:9) axis(2, pos = 0, at = -5:5) xd = seq(-8, 8, by = .1) lines(xd, 5/8 * sqrt(64 - xd^2), col = &#39;grey&#39;) lines(xd, 5/8 * -sqrt(64 - xd^2), col = &#39;grey&#39;) arrows(0, 0, x, y, col = &#39;red&#39;, length = .15, angle = 20) b = (x * 25) / (-y * 64) a = y - x * b abline(a, b, col = &#39;grey&#39;) b = -1/b x0 = x - y / b arrows(x0, 0, x, y, col = &#39;blue&#39;, length = .15, angle = 20) text(1.2, 0.5, label = parse(text = &quot;psi&quot;), col = &#39;red&#39;) text(3, 0.5, label = parse(text = &quot;phi&quot;), col = &#39;blue&#39;) Figure 2.3: Angles on an ellipse: geodetic (blue) and geocentric (red) latitude The most commonly used parametric model for the Earth is an ellipsoid of revolution, an ellipsoid with two equal semi-axes (Iliffe and Lott 2008). In effect, this is a flattened sphere (or spheroid): the distance between the poles is (slightly: about 0.33%) smaller than the distance between two opposite points on the equator. Under this model, longitude is always measured along a circle (as in figure 2.2), and latitude along an ellipse (as in figure 2.3). If we think of figure 2.3 as a cross section of the Earth passing through the poles, the geodetic latitude measure \\(\\phi\\) is the one used when no further specification is given. The latitude measure \\(\\psi\\) is called the geocentric latitude. In addition to longitude and latitude we can add altitude or elevation to define points that are not on the ellipsoid, and obtain a three dimensional space again. When defining altitude, we need to choose: where zero altitude is: on the ellipsoid, or relative to the surface approximating mean sea level (the geoid)? which direction is positive, and which direction is “straight up”: perpendicular to the ellipsoid surface, or in the direction perpendicular to the surface of the geoid? All these choices may matter, depending on the application area and required measurement accuracies. The shape of the Earth is not a perfect ellipsoid. As a consequence, several ellipsoids with different shape parameters and bound to the Earth in different ways are being used. Such ellipsoids are called datums, and are briefly discussed in section 2.3, along with coordinate reference systems. 2.2.2 Projected coordinates, distances Because paper maps and computer screens are much more abundant and practical than globes, most of the time we look at spatial data we see it projected: drawn on a flat, two-dimensional surface. Computing the locations in a two-dimensional space means that we work with projected coordinates. Projecting ellipsoidal coordinates means that shapes, directions, areas, or even all three, are distorted (Iliffe and Lott 2008). Distances between two points \\(p_i\\) and \\(p_j\\) in Cartesian coordinates are computed as Euclidean distances, in two dimensions by \\[d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\\] with \\(p_i = (x_i,y_i)\\) and in three dimensions by \\[d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}\\] with \\(p_i = (x_i,y_i,z_i).\\) These distances represent the length of a straight line between two points \\(i\\) and \\(j\\). For two points on a circle, the length of the arc of two points \\(c_1 = (r,{\\phi}_i)\\) and \\(c_2 = (r, \\phi_2)\\) is \\[s_{ij}=r~|\\phi_1-\\phi_2| = r ~\\theta\\] with \\(\\theta\\) the angle between \\(\\phi_1\\) and \\(\\phi_2\\) in radians. For very small values of \\(\\theta\\), we will have \\(s_{ij} \\approx d_{ij}\\), because a small arc segment is nearly straight. For two points \\(p_1 = (\\lambda_1,\\phi_1)\\) and \\(p_2 = (\\lambda_2,\\phi_2)\\) on a sphere with radius \\(r&#39;\\), the great circle distance is the arc length between \\(p_1\\) and \\(p_2\\) on the circle that passes through \\(p_1\\) and \\(p_2\\) and has the center of the sphere as its center, and is given by \\(s_{12} = r ~ \\theta_{12}\\) with \\[\\theta_{12} = \\arccos(\\sin \\phi_1 \\cdot \\sin \\phi_2 + \\cos \\phi_1 \\cdot \\cos \\phi_2 \\cdot \\cos(|\\lambda_1-\\lambda_2|))\\] the angle between \\(p_1\\) and \\(p_2\\), in radians. Arc distances between two points on a spheroid are more complicated to compute; a good discussion on the topic and an explanation of the method implemented in GeographicLib (part of PROJ) is given in Karney (2013). To show that these distance measures actually give different values, we computed them for the distance Berlin - Paris. Here, gc_ refers to ellipsoidal and spherical great circle distances, straight_ refers to straight line, Euclidean distances between Cartesian geocentric coordinates associated on the WGS84 ellipse and sphere: pts = st_sfc(st_point(c(13.4050, 52.5200)), st_point(c(2.3522, 48.8566)), crs = &#39;EPSG:4326&#39;) s2_orig = sf_use_s2(FALSE) d1 = c(gc_ellipse = st_distance(pts)[1,2]) sf_use_s2(TRUE) # or, without using s2, use st_distance(st_transform(pts, &quot;+proj=longlat +ellps=sphere&quot;)) d2 = c(gc_sphere = st_distance(pts)[1,2]) p = st_transform(pts, &quot;+proj=geocent&quot;) d3 = c(straight_ellipse = units::set_units(sqrt(sum(apply(do.call(cbind, p), 1, diff)^2)), m)) p2 = st_transform(pts, &quot;+proj=longlat +ellps=sphere&quot;) |&gt; st_transform(&quot;+proj=geocent&quot;) d4 = c(straight_sphere = units::set_units(sqrt(sum(apply(do.call(cbind, p2), 1, diff)^2)), m)) res = c(d1,d3,d2,d4) # print as km, re-add names: sf_use_s2(s2_orig) # back to what it was before changing res |&gt; units::set_units(km) |&gt; setNames(names(res)) |&gt; print(digits = 5) # Units: [km] # gc_ellipse straight_ellipse gc_sphere straight_sphere # 879.70 879.00 877.46 876.77 2.2.3 Bounded and unbounded spaces Two-dimensional and three-dimensional Euclidean spaces (\\(R^2\\) and \\(R^3\\)) are unbounded: every line in this space has infinite length, distances, areas or volumes are unbounded. In contrast, spaces defined on a circle (\\(S^1\\)) or sphere (\\(S^2\\)) define a bounded set: there may be infinitely many points but the length and area of the circle and the radius, area and volume of a sphere are bound. This may sound trivial, but leads to some interesting findings when handling spatial data. A polygon on \\(R^2\\) has unambiguously an inside and an outside. On a sphere, \\(S^2\\), any polygon divides the sphere in two parts, and which of these two is to be considered inside and which outside is ambiguous and needs to be defined e.g. by the traversal direction. Chapter ?? will further discuss consequences when working with geometries on \\(S^2\\). 2.3 Coordinate Reference Systems We follow Lott (2015) when defining the following concepts (italics indicate literal quoting): a coordinate system is a set of mathematical rules for specifying how coordinates are to be assigned to points, a datum is a parameter or set of parameters that define the position of the origin, the scale, and the orientation of a coordinate system, a geodetic datum is a datum describing the relationship of a two- or three-dimensional coordinate system to the Earth, and a coordinate reference system is a coordinate system that is related to an object by a datum; for geodetic and vertical datums, the object will be the Earth. A readable text that further explains these concepts is Iliffe and Lott (2008). The Earth does not follow a regular shape. The topography of the Earth is of course known to vary strongly, but also the surface formed by constant gravity at mean sea level, the geoid, is irregular. A commonly used model that is fit to the geoid is an ellipsoid of revolution, which is an ellipse with two identical minor axes. Fitting such an ellipsoid to the Earth gives a datum. However, fitting it to different areas, or based on different sets of reference points gives different fits, and hence different datums: a datum can for instance be fixed to a particular tectonic plate (like ETRS89), others can be globally fit (like WGS84). More local fits lead to smaller approximation errors. The definitions above imply that coordinates in degrees longitude and latitude only have a meaning, i.e. can only be interpreted unambiguously as Earth coordinates, when the datum they are associated with is given. Note that for projected data, the data that were projected are associated with a reference ellipsoid (datum). Going from one projection to another without changing datum is called coordinate conversion, and passes through the ellipsoidal coordinates associated with the datum involved. This process is lossless and invertible: the parameters and equations associated with a conversion are not empirical. Recomputing coordinates in a new datum is called coordinate transformation, and is approximate: because datums are a result of model fitting, transformations between datums are models too that have been fit; the equations involved are empirical, and multiple transformation paths, based on different model fits and associated with different accuracies, are possible. Plate tectonics imply that within a global datum, fixed objects may have coordinates that change over time, and that transformations from one datum to another may be time-dependent. Earthquakes are a cause of more local and sudden changes in coordinates. 2.4 PROJ and mapping accuracy Very few living people active in open source geospatial software can remember the time before PROJ. PROJ (Evenden 1990) started in the 1970s as a Fortran project, and was released in 1985 as a C library for cartographic projections. It came with command line tools for direct and inverse projections, and could be linked to software to let it support (re)projection directly. Originally, datums were considered implicit, and no datum transformations were allowed. In the early 2000s, PROJ was known as PROJ.4, after its never changing major version number. Amongst others motivated by the rise of GPS, the need for datum transformations increased and PROJ.4 was extended with rudimentary datum support. PROJ definitions for coordinate reference systems would look like this: +proj=utm +zone=33 +datum=WGS84 +units=m +no_defs where key=value pairs are preceded by a + and separated by a space. This form came to be known as “PROJ.4 string”, since the PROJ project stayed at version 4.x for several decades. Other datums would come with fields like: +ellps=bessel +towgs84=565.4,50.3,465.6,-0.399,0.344,-1.877,4.072 indicating another ellipse, as well as the seven (or three) parameters for transforming from this ellipse to WGS84 (the “World Geodetic System 1984” global datum once popularized by GPS), effectively defining the datum in terms of a transformation to WGS84. Along with PROJ.4 came a set of databases with known (registered) projections, from which the best known is the EPSG registry. National mapping agencies would provide (and update over time) their best guesses of +towgs84= parameters for national coordinate reference systems, and distribute it through the EPSG registry, which was part of PROJ distributions. For some transformations, datum grids were available and distributed as part of PROJ.4: such grids are raster maps that provide for every location pre-computed values for the shift in longitude and latitude, or elevation, for a particular datum transformation. library(stars) library(rnaturalearth) countries110 = st_as_sf(countries110) uk = countries110[countries110$admin %in% c(&quot;United Kingdom&quot;),] |&gt; st_geometry() r = read_stars(&quot;data/uk_os_OSTN15_NTv2_OSGBtoETRS.tif&quot;) # r = read_stars(&quot;/vsicurl/https://cdn.proj.org/uk_os_OSTN15_NTv2_OSGBtoETRS.tif&quot;) hook = function() { plot(uk, border = &quot;orange&quot;, col = NA, add = TRUE) } plot(r[,,,1:2], axes = TRUE, hook = hook, key.pos = 4) Figure 2.4: UK horizontal datum grid, from datum OSGB 1936 (EPSG:4277) to datum ETRS89 (EPSG:4258); units arc-seconds h = read_stars(&quot;data/uk_os_OSGM15_GB.tif&quot;) # h = read_stars(&quot;/vsicurl/https://cdn.proj.org/uk_os_OSGM15_GB.tif&quot;) plot(h, axes = TRUE, reset = FALSE) plot(uk, border = &quot;orange&quot;, col = NA, add = TRUE) Figure 2.5: UK vertical datum grid, from ETRS89 (EPSG:4937) to ODN height (EPSG:5701), units m In PROJ.4, every coordinate transformation had to go through a conversion to and from WGS84; even reprojecting data associated with a datum different from WGS84 had to go through a transformation to and from WGS84. The associated errors of up to 100 m were acceptable for mapping purposes for not too small areas, but applications that need high accuracy transformations, e.g. precision agriculture, planning flights of UAV’s, or object tracking are often more demanding in terms of accuracy. In 2018, after a successful “GDAL Coordinate System Barn Raising” initiative, a number of companies profiting from the open source geospatial software stack supported the development of a more modern, mature coordinate transformation system in PROJ. Over a few years, PROJ.4 evolved through versions 5, 6, 7 and 8 and was hence renamed into PROJ (or PR\\(\\phi\\)J). The most notable changes include: although PROJ.4 strings can still be used to initialize certain coordinate reference systems, they are no longer sufficient to represent all of them; a new format, WKT2 (described in next section) replaces it WGS84 as a hub datum is dropped: coordinate transformation no longer need to go through a particular datum multiple conversion or transformation paths (so-called pipelines) to go from CRS A to CRS B are possible, and can be reported along with the associated accuracy; PROJ will by default use the most accurate one but user control is possible transformation pipelines can chain an arbitrary number of elementary transformation operations, including swapping of axes and unit transformations datum grids, of which there are now many more, are no longer distributed with the library but are accessible from a content delivery network (CDN); PROJ allows to enabling and disabling network access to access these grids, and only downloads the section of the grid actually needed, storing it in a cache on the user’s machine for future use coordinate transformations receive support for epochs, time-dependent transformations (and hence: four-dimensional coordinates, including the source and target time) the set of files with registered coordinate reference systems is handled in an SQLite database instead of always handling axis order (longitude, latitude), when the authority defines differently this is now obeyed (with the most notable example: EPSG:4326 defines axis order to be latitude, longitude.) All these points sound like massive improvements, and accuracies of transformation can be below 1 metre. An interesting point is the last: Where we could safely assume for many decades that spatial data with ellipsoidal coordinates would have axis order (longitude, latitude), this is no longer the case. We will see in section ?? how to deal with this. Examples of a horizontal datum grids, downloaded from cdn.proj.org, are shown in figure 2.4 and for a vertical datum grid in figure 2.5. Datum grids may carry per-pixel accuracy values. 2.5 WKT-2 Lott (2015) describes a standard for encoding coordinate reference systems, as well as transformations between them using well known text; the standard (and format) is referred to informally as WKT-2. As mentioned above, GDAL and PROJ fully support this encoding. An example of WKT2 for CRS OGC:CRS84 is: GEOGCRS[&quot;WGS 84 (CRS84)&quot;, ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]], ENSEMBLEACCURACY[2.0]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;geodetic longitude (Lon)&quot;,east, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;geodetic latitude (Lat)&quot;,north, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], USAGE[ SCOPE[&quot;Not known.&quot;], AREA[&quot;World.&quot;], BBOX[-90,-180,90,180]], ID[&quot;OGC&quot;,&quot;CRS84&quot;]] This shows a coordinate system with the axis order longitude, latitude that can be used to replace EPSG:4326 when one wants unambiguously “traditional” (GIS) axis order. The ensemble of WGS84 ellipsoids listed represents its various updates over time. Ambiguity about which of these ensemble members a particular dataset should use leads to an uncerainty of several meters. A longer introduction on the history and recent changes in PROJ is given in Bivand (2020), building upon the work of Knudsen and Evers (2017) and Evers and Knudsen (2017). 2.6 Exercises Try to solve the following exercises with R (without loading packages); try to use functions where appropriate: list three geographic measures that do not have a natural zero origin convert the \\((x,y)\\) points \\((10,2)\\), \\((-10,-2)\\), \\((10,-2)\\) and \\((0,10)\\) to polar coordinates convert the polar \\((r,\\phi)\\) points \\((10,45^{\\circ})\\), \\((0,100^{\\circ})\\) and \\((5,359^{\\circ})\\) to Cartesian coordinates assuming the Earth is a sphere with a radius of 6371 km, compute for \\((\\lambda,\\phi)\\) points the great circle distance between \\((10,10)\\) and \\((11,10)\\), between \\((10,80)\\) and \\((11,80)\\), between \\((10,10)\\) and \\((10,11)\\) and between \\((10,80)\\) and \\((10,81)\\) (units: degree). What are the distance units? References "],["geometries.html", "Chapter 3 Geometries 3.1 Simple feature geometries 3.2 Operations on geometries 3.3 Precision 3.4 Coverages: tessellations and rasters", " Chapter 3 Geometries Having learned how we represent coordinates systems, we can define how geometries can be described using these coordinate systems. This chapter will explain: simple features, a standard that describes point, line and polygon geometries along with operations on them, operations on geometries coverages, subdivisions of larger regions into sub-regions networks Geometries on the sphere are discussed in chapter ??, rasters and other rectangular subdivisions of space are discussed in chapter ??. 3.1 Simple feature geometries Simple feature geometries are a way to describe the geometries of features. By features we mean things that have a geometry, potentially some time properties, and other attributes that could include a label describing the thing and quantitative measures of it. The main application of simple feature geometries is to describe geometries in two-dimensional space by points, lines, or polygons. The “simple” adjective refers to the fact that the line or polygon geometries are represented by sequences of points connected with straight lines that do not self-intersect. Simple features access is a standard (Herring 2011, 2010; ISO 2004) for describing simple feature geometries that includes: a class hierarchy a set of operations binary and text encodings We will first discuss the seven most common simple feature geometry types. 3.1.1 The big seven The most commonly used simple features geometries, used to represent a single feature are: type description POINT single point geometry MULTIPOINT set of points LINESTRING single linestring (two or more points connected by straight lines) MULTILINESTRING set of linestrings POLYGON exterior ring with zero or more inner rings, denoting holes MULTIPOLYGON set of polygons GEOMETRYCOLLECTION set of the geometries above library(sf) par(mfrow = c(2,4)) par(mar = c(1,1,1.2,1)) # 1 p = st_point(0:1) plot(p, pch = 16) title(&quot;point&quot;) box(col = &#39;grey&#39;) # 2 mp = st_multipoint(rbind(c(1,1), c(2, 2), c(4, 1), c(2, 3), c(1,4))) plot(mp, pch = 16) title(&quot;multipoint&quot;) box(col = &#39;grey&#39;) # 3 ls = st_linestring(rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3))) plot(ls, lwd = 2) title(&quot;linestring&quot;) box(col = &#39;grey&#39;) # 4 mls = st_multilinestring(list( rbind(c(1,1), c(5,5), c(5, 6), c(4, 6), c(3, 4), c(2, 3)), rbind(c(3,0), c(4,1), c(2,1)))) plot(mls, lwd = 2) title(&quot;multilinestring&quot;) box(col = &#39;grey&#39;) # 5 polygon po = st_polygon(list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)), rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2)))) plot(po, border = &#39;black&#39;, col = &#39;#ff8888&#39;, lwd = 2) title(&quot;polygon&quot;) box(col = &#39;grey&#39;) # 6 multipolygon mpo = st_multipolygon(list( list(rbind(c(2,1), c(3,1), c(5,2), c(6,3), c(5,3), c(4,4), c(3,4), c(1,3), c(2,1)), rbind(c(2,2), c(3,3), c(4,3), c(4,2), c(2,2))), list(rbind(c(3,7), c(4,7), c(5,8), c(3,9), c(2,8), c(3,7))))) plot(mpo, border = &#39;black&#39;, col = &#39;#ff8888&#39;, lwd = 2) title(&quot;multipolygon&quot;) box(col = &#39;grey&#39;) # 7 geometrycollection gc = st_geometrycollection(list(po, ls + c(0,5), st_point(c(2,5)), st_point(c(5,4)))) plot(gc, border = &#39;black&#39;, col = &#39;#ff6666&#39;, pch = 16, lwd = 2) title(&quot;geometrycollection&quot;) box(col = &#39;grey&#39;) Figure 3.1: sketches of the main simple feature geometry types Figure 3.1 shows examples of these basic geometry types. The human-readable, “well-known-text” (WKT) representation of the geometries plotted are: p mp ls mls po mpo gc POINT (0 1) MULTIPOINT ((1 1), (2 2), (4 1), (2 3), (1 4)) LINESTRING (1 1, 5 5, 5 6, 4 6, 3 4, 2 3) MULTILINESTRING ((1 1, 5 5, 5 6, 4 6, 3 4, 2 3), (3 0, 4 1, 2 1)) POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1), (2 2, 3 3, 4 3, 4 2, 2 2)) MULTIPOLYGON (((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1), (2 2, 3 3, 4 3, 4 2, 2 2)), ((3 7, 4 7, 5 8, 3 9, 2 8, 3 7))) GEOMETRYCOLLECTION ( POLYGON ((2 1, 3 1, 5 2, 6 3, 5 3, 4 4, 3 4, 1 3, 2 1), (2 2 , 3 3, 4 3, 4 2, 2 2)), LINESTRING (1 6, 5 10, 5 11, 4 11, 3 9, 2 8), POINT (2 5), POINT (5 4) ) In this representation, coordinates are separated by space, and points by commas. Sets are grouped by parentheses, and separated by commas. Individual points in a geometry contain at least two coordinates: x and y, in that order. If these coordinates refer to ellipsoidal coordinates, x and y usually refer to longitude and latitude, respectively, although sometimes to latitude and longitude (see sections 2.4 and ??). 3.1.2 Simple and valid geometries, ring direction Linestrings are called simple when they do not self-intersect: (ls = st_linestring(rbind(c(0,0), c(1,1), c(2,2), c(0,2), c(1,1), c(2,0)))) # LINESTRING (0 0, 1 1, 2 2, 0 2, 1 1, 2 0) c(is_simple = st_is_simple(ls)) # is_simple # FALSE Valid polygons and multipolygons obey all of the following properties: polygon rings are closed (the last point equals the first) polygon holes (inner rings) are inside their exterior ring polygon inner rings maximally touch the exterior ring in single points, not over a line a polygon ring does not repeat its own path in a multipolygon, an external ring maximally touches another exterior ring in single points, not over a line If this is not the case, the geometry concerned is not valid. Invalid geometries typically cause errors when they are processed, but can usually be repaired to make them valid. A further convention is that the outer ring of a polygon is winded counter-clockwise, while the holes are winded clockwise, but polygons for which this is not the case are still considered valid. For polygons on the sphere, the “clockwise” is not very useful: if for instance we take the equator as polygon, is the Northern hemisphere or the Southern hemisphere “inside”? The convention taken here is to consider the area on the left while traversing the polygon is considered the polygon’s inside. 3.1.3 Z and M coordinates In addition to X and Y coordinates, Single points (vertices) of simple feature geometries may have: a Z coordinate, denoting altitude, and/or an M value, denoting some “measure” The M attribute shall be a property of the vertex. It sounds attractive to encode a time stamp in it, e.g. to pack movement data (trajectories) in LINESTRINGs. These become however invalid (or “non-simple”) once the trajectory self-intersects, which easily happens when only X and Y are considered for self-intersections. Both Z and M are not found often, and software support to do something useful with them is (still) rare. Their WKT representation are fairly easily understood: st_point(c(1,3,2)) # POINT Z (1 3 2) st_point(c(1,3,2), dim = &quot;XYM&quot;) # POINT M (1 3 2) st_linestring(rbind(c(3,1,2,4), c(4,4,2,2))) # LINESTRING ZM (3 1 2 4, 4 4 2 2) 3.1.4 Empty geometries A very important concept in the feature geometry framework is that of the empty geometry. Empty geometries arise naturally when we do geometrical operations (section 3.2), for instance when we want to know the intersection of POINT (0 0) and POINT (1 1): (e = st_intersection(st_point(c(0,0)), st_point(c(1,1)))) # GEOMETRYCOLLECTION EMPTY and it represents essentially the empty set: when combining (unioning) an empty point it with other non-empty geometries, it vanishes. All geometry types have a special value representing the empty (typed) geometry: st_point() # POINT EMPTY st_linestring(matrix(1,1,3)[0,], dim = &quot;XYM&quot;) # LINESTRING M EMPTY and so on, but they all point to the empty set, differing only in their dimension (section 3.2.2). 3.1.5 Ten further geometry types There are 10 more geometry types which are more rare, but increasingly find implementation: type description CIRCULARSTRING The CIRCULARSTRING is the basic curve type, similar to a LINESTRING in the linear world. A single segment requires three points, the start and end points (first and third) and any other point on the arc. The exception to this is for a closed circle, where the start and end points are the same. In this case the second point MUST be the center of the arc, i.e. the opposite side of the circle. To chain arcs together, the last point of the previous arc becomes the first point of the next arc, just like in LINESTRING. This means that a valid circular string must have an odd number of points greater than 1. COMPOUNDCURVE A compound curve is a single, continuous curve that has both curved (circular) segments and linear segments. That means that in addition to having well-formed components, the end point of every component (except the last) must be coincident with the start point of the following component. CURVEPOLYGON Example compound curve in a curve polygon: CURVEPOLYGON(COMPOUNDCURVE(CIRCULARSTRING(0 0,2 0, 2 1, 2 3, 4 3),(4 3, 4 5, 1 4, 0 0)), CIRCULARSTRING(1.7 1, 1.4 0.4, 1.6 0.4, 1.6 0.5, 1.7 1) ) MULTICURVE A MultiCurve is a 1-dimensional GeometryCollection whose elements are Curves, it can include linear strings, circular strings or compound strings. MULTISURFACE A MultiSurface is a 2-dimensional GeometryCollection whose elements are Surfaces, all using coordinates from the same coordinate reference system. CURVE A Curve is a 1-dimensional geometric object usually stored as a sequence of Points, with the subtype of Curve specifying the form of the interpolation between Points SURFACE A Surface is a 2-dimensional geometric object POLYHEDRALSURFACE A PolyhedralSurface is a contiguous collection of polygons, which share common boundary segments TIN A TIN (triangulated irregular network) is a PolyhedralSurface consisting only of Triangle patches. TRIANGLE A Triangle is a polygon with 3 distinct, non-collinear vertices and no interior boundary CIRCULARSTRING, COMPOUNDCURVE and CURVEPOLYGON are not described in the SFA standard, but in the SQL-MM part 3 standard. The descriptions above were copied from the PostGIS manual. 3.1.6 Text and binary encodings Part of the simple feature standard are two encodings: a text and a binary encoding. The well-known text encoding, used above, is human-readable, the well-known binary encoding is machine-readable. Binary encodings are lossless and typically faster to work with than text encoding (and decoding), and are used for instance in all communications between R package sf and the GDAL, GEOS, liblwgeom and s2geometry libraries (figure 1.6). 3.2 Operations on geometries Simple feature geometries can be queried for properties, transformed into new geometries, and combinations of geometries can be queried for properties. This section gives an overview of the operations entirely focusing on geometrical properties. Chapter ?? focuses on the analysis of non-geometrical feature properties, in relationship to their geometries. Some of the material in this section appeared in Pebesma (2018). We can categorize operations on geometries in terms of what they take as input, and what they return as output. In terms of output we have operations that return: predicates: a logical asserting a certain property is TRUE measures: a quantity (e.g. a numeric value with measurement unit) transformations: newly generated geometries and in terms of what they operate on, we distinguish operations that are: unary when they work on a single geometry binary when they work on pairs of geometries n-ary when they work on sets of geometries 3.2.1 Unary predicates Unary predicates describe a certain property of a geometry. The predicates is_simple, is_valid, and is_empty return respectively whether a geometry is simple, valid or empty. Given a coordinate reference system, is_longlat returns whether the coordinates are geographic or projected. is(geometry, class) checks whether a geometry belongs to a particular class. 3.2.2 Binary predicates and DE-9IM The Dimensionally Extended Nine-Intersection Model (DE-9IM, Clementini, Di Felice, and Oosterom (1993); Egenhofer and Franzosa (1991)) is a model that helps describing the qualitative relation between any two geometries in two-dimensional space (\\(R^2\\)). Any geometry has a dimension value that is: 0 for points, 1 for linear geometries, 2 for polygonal geometries, and F (false) for empty geometries Any geometry also has an inside (I), a boundary (B) and an exterior (E); these roles are obvious for polygons but, e.g. for: lines the boundary is formed by the end points, and the interior by all non-end points on the line points have a zero-dimensional inside but no boundary library(sf) polygon = po = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,1), c(0,0)))) p0 = st_polygon(list(rbind(c(-1,-1), c(2,-1), c(2,2), c(-1,2), c(-1,-1)))) line = li = st_linestring(rbind(c(.5, -.5), c(.5, 0.5))) s = st_sfc(po, li) par(mfrow = c(3,3)) par(mar = c(1,1,1,1)) # &quot;1020F1102&quot; # 1: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;I(line) = 1&quot;))) lines(rbind(c(.5,0), c(.5,.495)), col = &#39;red&#39;, lwd = 2) points(0.5, 0.5, pch = 1) # 2: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;B(line) = 0&quot;))) points(0.5, 0.5, col = &#39;red&#39;, pch = 16) # 3: 2 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;I(pol)&quot;,intersect(),&quot;E(line) = 2&quot;))) plot(po, col = &#39;#ff8888&#39;, add = TRUE) plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, add = TRUE) # 4: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;I(line) = 0&quot;))) points(.5, 0, col = &#39;red&#39;, pch = 16) # 5: F plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;B(line) = F&quot;))) # 6: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;B(pol)&quot;,intersect(),&quot;E(line) = 1&quot;))) plot(po, border = &#39;red&#39;, col = NA, add = TRUE, lwd = 2) # 7: 1 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;I(line) = 1&quot;))) lines(rbind(c(.5, -.5), c(.5, 0)), col = &#39;red&#39;, lwd = 2) # 8: 0 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;B(line) = 0&quot;))) points(.5, -.5, col = &#39;red&#39;, pch = 16) # 9: 2 plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, main = expression(paste(&quot;E(pol)&quot;,intersect(),&quot;E(line) = 2&quot;))) plot(p0 / po, col = &#39;#ff8888&#39;, add = TRUE) plot(s, col = c(NA, &#39;darkgreen&#39;), border = &#39;blue&#39;, add = TRUE) Figure 3.2: DE-9IM: intersections between the interior, boundary and exterior of a polygon (rows) and of a linestring (columns) indicated by red Figure 3.2 shows the intersections between the I, B and E components of a polygon and a linestring indicated by red; the sub-plot title gives the dimension of these intersections (0, 1, 2 or F). The relationship between the polygon and the line geometry is the concatenation of these dimensions: st_relate(polygon, line) # [,1] # [1,] &quot;1020F1102&quot; where the first three characters are associated with the inside of the first geometry (the polygon): figure 3.2 is summarised row-wise. Using this ability to express relationships, we can also query pairs of geometries about particular conditions expressed in a mask string; e.g. the string \"*0*******\" would evaluate TRUE when the second geometry has one or more boundary points in common with the interior of the first geometry; the symbol * standing for “any dimensionality” (0, 1, 2 or F). The mask string \"T********\" matches pairs of geometry with intersecting interiors, where the symbol T stands for any non-empty intersection of dimensionality 0, 1 or 2. Binary predicates are further described using normal-language verbs, using DE-9IM definitions. For instance, the predicate equals corresponds to the relationship \"T*F**FFF*\". If any two geometries obey this relationship, they are (topologically) equal, but may have a different ordering of nodes. A list of binary predicates is: predicate meaning inverse of contains None of the points of A are outside B within contains_properly A contains B and B has no points in common with the boundary of A covers No points of B lie in the exterior of A covered_by covered_by Inverse of covers crosses A and B have some but not all interior points in common disjoint A and B have no points in common intersects equals A and B are topologically equal: node order or number of nodes may differ; identical to A contains B AND A within B equals_exact A and B are geometrically equal, and have identical node order intersects A and B are not disjoint disjoint is_within_distance A is closer to B than a given distance within None of the points of B are outside A contains touches A and B have at least one boundary point in common, but no interior points overlaps A and B have some points in common; the dimension of these is identical to that of A and B relate given a mask pattern, return whether A and B adhere to this pattern The Wikipedia DE-9IM page provides the relate patterns for each of these verbs. They are important to check out; for instance covers and contains (and their inverses) are often not completely intuitive: if A contains B, B has no points in common with the exterior or boundary of A if A covers B, B has no points in common with the exterior of A 3.2.3 Unary Measures Unary measures return a measure or quantity that describes a property of the geometry: measure returns dimension 0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries area the area of a geometry length the length of a linear geometry 3.2.4 Binary Measures distance returns the distance between pairs of geometries. The qualitative measure relate (without mask) gives the relation pattern, a description of the geometrical relationship between two geometries explained in section 3.2.2. 3.2.5 Unary Transformers Unary transformations work on a per-geometry basis, and for each geometry return a new geometry. transformer returns a geometry … centroid of type POINT with the geometry’s centroid buffer that is this larger (or smaller) than the input geometry, depending on the buffer size jitter that was moved in space a certain amount, using a bivariate uniform distribution wrap_dateline cut into pieces that do no longer cover the dateline boundary with the boundary of the input geometry convex_hull that forms the convex hull of the input geometry (figure 3.3) line_merge after merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs. make_valid that is valid node with added nodes to linear geometries at intersections without a node; only works on individual linear geometries point_on_surface with a (arbitrary) point on a surface polygonize of type polygon, created from lines that form a closed ring segmentize a (linear) geometry with nodes at a given density or minimal distance simplify simplified by removing vertices/nodes (lines or polygons) split that has been split with a splitting linestring transform transformed or convert to a new coordinate reference system (chapter 2) triangulate with Delauney triangulated polygon(s) (figure 3.3) voronoi with the Voronoi tessellation of an input geometry (figure 3.3) zm with removed or added Z and/or M coordinates collection_extract with subgeometries from a GEOMETRYCOLLECTION of a particular type cast that is converted to another type + that is shifted over a given vector * that is multiplied by a scalar or matrix par(mar = rep(0,4), mfrow = c(1, 3)) set.seed(133331) mp = st_multipoint(matrix(runif(20), 10)) plot(mp, cex = 2) plot(st_convex_hull(mp), add = TRUE, col = NA, border = &#39;red&#39;) box() plot(mp, cex = 2) plot(st_voronoi(mp), add = TRUE, col = NA, border = &#39;red&#39;) box() plot(mp, cex = 2) plot(st_triangulate(mp), add = TRUE, col = NA, border = &#39;darkgreen&#39;) box() Figure 3.3: for a set of points, left: convex hull (red); middle: Voronoi polygons; right: Delauney triangulation 3.2.6 Binary Transformers Binary transformers are functions that return a geometry based on operating on a pair of geometries. They include: function returns infix operator intersection the overlapping geometries for pair of geometries &amp; union the combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces | difference the geometries of the first after removing the overlap with the second geometry / sym_difference the combinations of the geometries after removing where they intersect; the negation (opposite) of intersection %/% 3.2.7 N-ary Transformers N-ary transformers operate on sets of geometries. union can be applied to a set of geometries to return its geometrical union. Otherwise, any set of geometries can be combined into a MULTI-type geometry when they have equal dimension, or else into a GEOMETRYCOLLECTION. Without unioning, this may lead to a geometry that is not valid, e.g. because two polygon rings have a boundary line in common. N-ary intersection and difference take a single argument, but operate (sequentially) on all pairs, triples, quadruples, etc. Consider the plot in figure 3.4: how do we identify the area where all three boxes overlap? Using binary intersections gives us intersections for all pairs: 1-1, 1-2, 1-3, 2-1, 2-2, 2-3, 3-1, 3-2, 3-3, but does not let us identify areas where more than two geometries intersect. Figure 3.4 (right) shows the n-ary intersection: the 7 unique, non-overlapping geometries originating from intersection of one, two, or more geometries. par(mar = rep(.1, 4), mfrow = c(1, 2)) sq = function(pt, sz = 1) st_polygon(list(rbind(c(pt - sz), c(pt[1] + sz, pt[2] - sz), c(pt + sz), c(pt[1] - sz, pt[2] + sz), c(pt - sz)))) x = st_sf(box = 1:3, st_sfc(sq(c(0,0)), sq(c(1.7, -0.5)), sq(c(0.5, 1)))) plot(st_geometry(x), col = NA, border = sf.colors(3, categorical = TRUE), lwd = 3) plot(st_intersection(st_geometry(x)), col = sf.colors(7, categorical=TRUE, alpha = .5)) Figure 3.4: left: three overlapping boxes – how do we identify the small box where all three overlap? right: unique, non-overlapping n-ary intersections Similarly, one can compute an n-ary difference from a set \\(\\{s_1, s_2, s_3, ...\\}\\) by creating differences \\(\\{s_1, s_2-s_1, s_3-s_2-s_1, ...\\}\\). This is shown in figure 3.5, left for the original set, right for the set after reversing its order to make clear that the result here depends on the ordering of the input geometries. Again, resulting geometries do not overlap. par(mar = rep(.1, 4), mfrow = c(1, 2)) xg = st_geometry(x) plot(st_difference(xg), col = sf.colors(3, alpha = .5, categorical=TRUE)) plot(st_difference(xg[3:1]), col = sf.colors(3, alpha = .5, categorical=TRUE)) Figure 3.5: difference between subsequent boxes, left: in original order; right: in reverse order 3.3 Precision Geometrical operations, such as finding out whether a certain point is on a line, may fail when coordinates are represented by double precision floating point numbers, such as 8-byte doubles used in R. An often chosen remedy is to limit the precision of the coordinates before the operation. For this, a precision model is adopted; the most common is to choose a factor \\(p\\) and compute rounded coordinates \\(c&#39;\\) from original coordinates \\(c\\) by \\[c&#39; = \\mbox{round}(p \\cdot c) / p\\] Rounding of this kind brings the coordinates to points on a regular grid with spacing \\(1/p\\), which is beneficial for geometric computations. Of course, it also affects all computations like areas and distances, and may turn valid geometries into invalid ones. Which precision values are best for which application is often a matter of common sense combined with trial and error. 3.4 Coverages: tessellations and rasters The Open Geospatial Consortium defines a coverage as a “feature that acts as a function to return values from its range for any direct position within its spatiotemporal domain” (Baumann, Hirschorn, and Masó 2017). Having a function implies that for every “point”, i.e. every combination of spatial point and a moment in time of the spatiotemporal domain, we have single value for the range. This is a very common situation for spatiotemporal phenomena, a few examples can be given: boundary disputes aside, every point in a region (domain) belongs to a single administrative unit (range) at any given moment in time, every point in a region (domain) has a certain land cover type (range) every point in an area (domain) has a single elevation (range), e.g. measured with respect to a given mean sea level surface every spatiotemporal point in a three-dimensional body of air (domain) has single value for temperature (range) A caveat here is that because observation or measurement always takes time and requires space, measured values are always an average over a spatiotemporal volume, and hence range variables can rarely be measured for true, zero-volume “points”; for many practical cases however the measured volume is small enough to be considered a “point”; for a variable like land cover type the volume needs to be chosen such that the types distinguished make sense with respect to the measured units. In the first two of the given examples the range variable is categorical, in the last two the range variable is continuous. For categorical range variables, if large connected areas have a constant range value, an efficient way to represent these data is by storing the boundaries of the areas with constant value, such as country boundaries. Although this can be done (and is often done) by a set of simple feature geometries (polygons or multipolygons), but this brings along some challenges: it is hard to guarantee for such a set of simple feature polygons that they do not overlap, or that there are no gaps between them simple features have no way of assigning points on the boundary of two adjacent polygons uniquely to a single polygon, which introduces ambiguity in terms the interpretation as coverage 3.4.1 Topological models A data model that guarantees no inadvertent gaps or overlaps of polygonal coverages is the topological model, examples of which are found in geographic information systems (GIS) like GRASS GIS or ArcGIS. Topological models store boundaries between polygons only once, and register which polygonal area is on either side of a boundary. Deriving the set of (multi)polygons for each area with a constant range value from a topological model is straightforward; the other way around: reconstructing topology from a set of polygons typically involves setting thresholds on errors and handling gaps or overlaps. 3.4.2 Raster tessellations A tessellation is a subdivision of a space (area, volume) into smaller elements by ways of polygons. A regular tessellation does this with regular polygons: triangles, squares or hexagons. Tessellations using squares are very commonly used for spatial data, and are called raster data. Raster data tessellate each spatial dimension \\(d\\) into regular cells, formed e.g. by left-closed and right-open intervals \\(d_i\\): \\[\\begin{equation} d_i = d_0 + [i \\times \\delta, (i+1) \\times \\delta) \\end{equation}\\] with \\(d_0\\) an offset, \\(\\delta\\) the interval (cell or pixel) size, and where the cell index \\(i\\) is an arbitrary but consecutive set of integers. The \\(\\delta\\) value is often taken negative for the \\(y\\)-axis (Northing), indicating that raster row numbers increasing Southwards correspond to \\(y\\)-coordinates increasing Northwards. Where in arbitrary polygon tessellations the assignment of points to polygons is ambiguous for points falling on a boundary shared by two polygons, using left-closed “[” and right-open ”)” intervals in regular tessellations removes this ambiguity. This means that for rasters with negative \\(\\delta\\) values for the \\(y\\)-coordinate and positive for the \\(x\\)-coordinate, only the top-left corner point is part of each raster cell. An artifact resulting from this is shown in figure ??.] cat(st_crs(”EPSG:4326”)$wkt, “”) st_axis_order(TRUE) tif = system.file(“tif/L7_ETMs.tif”, package = “stars”) read_stars(tif) |&gt; st_transform(4326) read_stars(tif) |&gt; st_warp(crs = st_crs(4326)) |&gt; st_dimensions() r = read_stars(tif) grd = st_bbox(r) |&gt; st_as_sfc() |&gt; st_transform(4326) |&gt; st_bbox() |&gt; st_as_stars(nx = dim(r)[“x”], ny = dim(r)[“y”]) st_warp(r, grd) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) References "],["for-units.html", "Chapter 4 for units: (?)", " Chapter 4 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) (file = system.file(”gpkg/nc.gpkg”, package = “sf”)) bb = “POLYGON ((-81.7 36.2, -80.4 36.2, -80.4 36.5, -81.7 36.5, -81.7 36.2))” nc.1 = st_read(file, wkt_filter = bb) q = paste(“select BIR74,SID74,geom from ‘nc.gpkg’ where BIR74 &gt; 1500”) nc.2 = st_read(file, query = q) q = paste(“select BIR74,SID74,geom from ‘nc.gpkg’ LIMIT 10 OFFSET 50”) nc.2 = st_read(file, query = q) has_PG &lt;- any(“PostgreSQL” %in% st_drivers()$name) &amp;&amp; !inherits(try(DBI::dbConnect( RPostgres::Postgres(), host = “localhost”, dbname = “postgis”), silent = TRUE), “try-error”) nc = read_sf(file) write_sf(nc, “PG:dbname=postgis”, “nc”) pg &lt;- DBI::dbConnect( RPostgres::Postgres(), host = “localhost”, dbname = “postgis”) st_read(pg, query = “select BIR74,wkb_geometry from nc limit 3”) q = “SELECT BIR74,wkb_geometry FROM nc WHERE ST_Intersects(wkb_geometry, ‘SRID=4267;POINT (-81.49826 36.4314)’);” st_read(pg, query = q) library(dplyr, warn.conflicts = FALSE) nc_db = tbl(pg, “nc”) nc_db |&gt; filter(ST_Intersects(wkb_geometry, ‘SRID=4267;POINT (-81.49826 36.4314)’)) |&gt; collect() nc_db |&gt; filter(ST_Area(wkb_geometry) &gt; 0.1) |&gt; head(3) download.file( “https://openstreetmap.org/api/0.6/map?bbox=7.595,51.969,7.598,51.970”, “data/ms.osm”, method = “auto”) o = read_sf(“data/ms.osm”, “lines”) p = read_sf(“data/ms.osm”, “multipolygons”) bb = st_bbox(c(xmin=7.595, ymin = 51.969, xmax = 7.598, ymax = 51.970), crs = 4326) plot(st_as_sfc(bb), axes = TRUE, lwd = 2, lty = 2, cex.axis = .5) plot(o[,1], lwd = 2, add = TRUE) plot(st_geometry(p), border = NA, col = ‘#88888888’, add = TRUE) options(timeout = 600) # or large in case of slow network install.packages(“starsdata”, repos = “http://pebesma.staff.ifgi.de”, type = “source”) f = “sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip” granule = system.file(file = f, package = “starsdata”) file.size(granule) base_name = strsplit(basename(granule), “.zip”)[[1]] s2 = paste0(“SENTINEL2_L1C:/vsizip/”, granule, “/”, base_name, “.SAFE/MTD_MSIL1C.xml:10m:EPSG_32632”) (p = read_stars(s2, proxy = TRUE)) object.size(p) p2 = p * 2 plot(p) (ds = floor(sqrt(prod(dim(p)) / prod(dev.size(“px”))))) methods(class = “stars_proxy”) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-1.html", "Chapter 5 for units: (?)", " Chapter 5 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) library(rnaturalearth) w &lt;- ne_countries(scale =”medium”, returnclass = “sf”) suppressWarnings(st_crs(w) &lt;- st_crs(4326)) layout(matrix(1:2, 1, 2), c(2,1)) par(mar = rep(0, 4)) plot(st_geometry(w)) "],["sphere.html", "Chapter 6 sphere:", " Chapter 6 sphere: library(s2) g = as_s2_geography(TRUE) # Earth co = s2_data_countries() oc = s2_difference(g, s2_union_agg(co)) # oceans b = s2_buffer_cells(as_s2_geography(“POINT(-30 -10)”), 9800000) # visible half i = s2_intersection(b, oc) # visible ocean co = s2_intersection(b, co) plot(st_transform(st_as_sfc(i), “+proj=ortho +lat_0=-10 +lon_0=-30”), col = ‘lightblue’) plot(st_transform(st_as_sfc(co), “+proj=ortho +lat_0=-10 +lon_0=-30”), col = NA, add = TRUE) library(sf) library(rnaturalearth) w &lt;- ne_countries(scale = “medium”, returnclass = “sf”) plot(st_geometry(w)) st_is_longlat(w) DE = st_geometry(ne_countries(country = “germany”, returnclass = “sf”)) DE.eqc = st_transform(DE, “+proj=eqc +lat_ts=51.14 +lon_0=90w”) print(mean(st_bbox(DE)[c(“ymin”, “ymax”)]), digits = 4) par(mfrow = c(1, 2)) plot(DE, axes = TRUE) plot(DE.eqc, axes = TRUE) library(classInt) # set.seed(1) needed ? r = rnorm(100) (cI &lt;- classIntervals(r)) cI$brks library(sf) nc = read_sf(system.file(“gpkg/nc.gpkg”, package = “sf”)) plot(nc[“BIR74”], reset = FALSE, key.pos = 4) plot(st_buffer(nc[1,1], units::set_units(10, km)), col = ‘NA’, border = ‘red’, lwd = 2, add = TRUE) library(stars) r = read_stars(system.file(“tif/L7_ETMs.tif”, package = “stars”)) circ = st_bbox(r) |&gt; st_as_sfc() |&gt; st_sample(5) |&gt; st_buffer(300) hook = function() plot(circ, col = NA, border = ‘yellow’, add = TRUE) plot(r, hook = hook, key.pos = 4) suppressPackageStartupMessages(library(tidyverse)) nc.32119 = st_transform(nc, 32119) year_labels = c(“SID74” = “1974 - 1978”, “SID79” = “1979 - 1984”) nc.32119 |&gt; select(SID74, SID79) |&gt; pivot_longer(starts_with(“SID”)) -&gt; nc_longer ggplot() + geom_sf(data = nc_longer, aes(fill = value)) + facet_wrap(~ name, ncol = 1, labeller = labeller(name = year_labels)) + scale_y_continuous(breaks = 34:36) + scale_fill_gradientn(colours = sf.colors(20)) + theme(panel.grid.major = element_line(colour = “white”)) library(ggplot2) library(stars) r = read_stars(system.file(“tif/L7_ETMs.tif”, package = “stars”)) ggplot() + geom_stars(data = r) + facet_wrap(~band) + coord_equal() + theme_void() + scale_x_discrete(expand = c(0,0)) + scale_y_discrete(expand = c(0,0)) + scale_fill_viridis_c() library(tmap) system.file(“gpkg/nc.gpkg”, package = “sf”) |&gt; read_sf() |&gt; st_transform(‘EPSG:32119’) -&gt; nc.32119 tm_shape(nc.32119) + tm_polygons(c(“SID74”, “SID79”)) nc.32119 |&gt; select(SID74, SID79) |&gt; pivot_longer(starts_with(“SID”), values_to = “SID”) -&gt; nc_longer tm_shape(nc_longer) + tm_polygons(“SID”) + tm_facets(by = “name”) nc.32119 |&gt; select(SID74, SID79) |&gt; pivot_longer(starts_with(“SID”), values_to = “SID”) -&gt; nc_longer tm_shape(nc_longer) + tm_polygons(“SID”) + tm_facets(by = “name”) tm_shape(r) + tm_raster() tm_shape(r) + tm_raster() tmap_mode(“view”) tmap_mode(“plot”) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-2.html", "Chapter 7 for units: (?)", " Chapter 7 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-3.html", "Chapter 8 for units: (?)", " Chapter 8 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sf) n = 30 set.seed(13531) # remove this to create another random sequence xy = data.frame(x = runif(n), y = runif(n)) |&gt; st_as_sf(coords = c(”x”, “y”)) w1 = st_bbox(c(xmin = 0, ymin = 0, xmax = 1, ymax = 1)) |&gt; st_as_sfc() w2 = st_sfc(st_point(c(1, 0.5))) |&gt; st_buffer(1.2) par(mfrow = c(1, 2), mar = c(2.1, 2.1, 0.1, 0.5), xaxs = “i”, yaxs = “i”) plot(w1, axes = TRUE, col = ‘grey’) plot(xy, add = TRUE) plot(w2, axes = TRUE, col = ‘grey’) plot(xy, add = TRUE, cex = .5) suppressPackageStartupMessages(library(spatstat)) as.ppp(xy) (pp1 = c(w1, st_geometry(xy)) |&gt; as.ppp()) c1 = st_buffer(st_centroid(w2), 1.2) (pp2 = c(c1, st_geometry(xy)) |&gt; as.ppp()) par(mfrow = c(1, 2), mar = rep(0, 4)) q1 = quadratcount(pp1, nx=3, ny=3) q2 = quadratcount(pp2, nx=3, ny=3) plot(q1, main = ““) plot(xy, add = TRUE) plot(q2, main =”“) plot(xy, add = TRUE) quadrat.test(pp1, nx=3, ny=3) quadrat.test(pp2, nx=3, ny=3) den1 &lt;- density(pp1, sigma = bw.diggle) den2 &lt;- density(pp2, sigma = bw.diggle) par(mfrow = c(1, 2), mar = c(0,0,1.1,2)) plot(den1) plot(pp1, add=TRUE) plot(den2) plot(pp1, add=TRUE) library(stars) s1 = st_as_stars(den1) (s2 = st_as_stars(den2)) sum(s1[[1]], na.rm = TRUE)st_dimensions(s1)\\(x\\)delta^2 sum(s2[[1]], na.rm = TRUE)st_dimensions(s2)\\(x\\)delta^2 pt = st_sfc(st_point(c(0.5, 0.5))) s2\\(dist = st_distance(st_as_sf(s2, as_points = TRUE, na.rm = FALSE), pt) (m = ppm(pp2 ~ dist, data = list(dist = as.im(s2[&quot;dist&quot;])))) plot(m, se = FALSE) predict(m, covariates = list(dist = as.im(s2[&quot;dist&quot;]))) |&gt; st_as_stars() system.file(&quot;gpkg/nc.gpkg&quot;, package = &quot;sf&quot;) |&gt; read_sf() |&gt; st_geometry() |&gt; st_centroid() |&gt; as.ppp() longleaf ll = st_as_sf(longleaf) print(ll, n = 5) as.ppp(ll) print(st_as_sf(copper\\)SouthLines), n = 5) print(st_as_sf(chicago), n = 5) table(st_as_sf(chicago)$label) kappa = 30 / st_area(w2) # intensity th = st_sample(w2, kappa = kappa, mu = 3, scale = 0.05, type =”Thomas”) nrow(th) par(mar = rep(0, 4)) plot(w2) plot(th, add = TRUE) # example from plotting chapter: par(mar = rep(0, 4)) library(s2) g = as_s2_geography(TRUE) # Earth co = s2_data_countries() oc = s2_difference(g, s2_union_agg(co)) # oceans b = s2_buffer_cells(as_s2_geography(“POINT(-30 -10)”), 9800000) # visible half i = s2_intersection(b, oc) # visible ocean co = s2_intersection(b, co) plot(st_transform(st_as_sfc(i), “+proj=ortho +lat_0=-10 +lon_0=-30”), col = ‘lightblue’) plot(st_transform(st_as_sfc(co), “+proj=ortho +lat_0=-10 +lon_0=-30”), col = NA, add = TRUE, border = ‘grey’) # sampling from globe: #sf_use_s2(FALSE) assign(“.sf.use_s2”, FALSE, envir=sf:::.sf_cache) # cheat! pts = suppressMessages( # cheat! st_sample(st_as_sfc(st_bbox(st_as_stars())), 1000, exact = FALSE)) #sf_use_s2(TRUE) assign(“.sf.use_s2”, TRUE, envir = sf:::.sf_cache) # cheat! pts = s2_intersection(i, pts) |&gt; st_as_sfc() plot(st_transform(pts, “+proj=ortho +lat_0=-10 +lon_0=-30”), add = TRUE, pch = 3, cex = .7) # we need to detach, to avoid name clashes of using idw() and diagnose() later on library(spatstat) w = function(x) which(x == search()) detach(pos = w(“package:spatstat”)) detach(pos = w(“package:spatstat.linnet”)) detach(pos = w(“package:spatstat.core”)) detach(pos = w(“package:spatstat.random”)) detach(pos = w(“package:spatstat.geom”)) detach(pos = w(“package:spatstat.data”)) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-4.html", "Chapter 9 for units: (?)", " Chapter 9 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) # after running the”Spatiotemporal Geostatistics” chapter, write NO2 means by write_csv(right_join(a2, tb), “no2.csv”) library(tidyverse) no2 = read_csv(system.file(“external/no2.csv”, package = “gstat”)) library(sf) crs = st_crs(“EPSG:32632”) no2.sf = st_as_sf(no2, coords = c(“station_longitude_deg”, “station_latitude_deg”), crs = “OGC:CRS84”) |&gt; st_transform(crs) data(air, package = “spacetime”) # this loads German boundaries into DE_NUTS1 de &lt;- st_transform(st_as_sf(DE_NUTS1), crs) ggplot() + geom_sf(data = de) + geom_sf(data = no2.sf, mapping = aes(col = NO2)) library(stars) st_bbox(de) |&gt; st_as_stars(dx = 10000) |&gt; st_crop(de) -&gt; grd grd library(gstat) i = idw(NO2~1, no2.sf, grd) ggplot() + geom_stars(data = i, aes(fill = var1.pred, x = x, y = y)) + geom_sf(data = st_cast(de, “MULTILINESTRING”)) + geom_sf(data = no2.sf) v = variogram(NO2~1, no2.sf) plot(v, plot.numbers = TRUE) library(gstat) v0 = variogram(NO2~1, no2.sf, cutoff = 100000, width = 10000) plot(v0, plot.numbers = TRUE) v.m = fit.variogram(v, vgm(1, “Exp”, 50000, 1)) plot(v, v.m, plot.numbers = TRUE) k = krige(NO2~1, no2.sf, grd, v.m) ggplot() + geom_stars(data = k, aes(fill = var1.pred, x = x, y = y)) + geom_sf(data = st_cast(de, “MULTILINESTRING”)) + geom_sf(data = no2.sf) + coord_sf(lims_method = “geometry_bbox”) a = aggregate(no2.sf[“NO2”], by = de, FUN = mean) b = krige(NO2~1, no2.sf, de, v.m) b\\(sample = a\\)NO2 b\\(kriging = b\\)var1.pred b |&gt; select(sample, kriging) |&gt; pivot_longer(1:2, names_to = “var”, values_to = “NO2”) -&gt; b2 b2\\(var = factor(b2\\)var, levels = c(“sample”, “kriging”)) ggplot() + geom_sf(data = b2, mapping = aes(fill = NO2)) + facet_wrap(~var) + scale_fill_gradientn(colors = sf.colors(20)) SE = function(x) sqrt(var(x)/length(x)) a = aggregate(no2.sf[“NO2”], de, SE) b\\(sample = a\\)NO2 b\\(kriging = sqrt(b\\)var1.var) b |&gt; select(sample, kriging) |&gt; pivot_longer(1:2, names_to = “var”, values_to = “Standard_error”) -&gt; b2 b2\\(var = factor(b2\\)var, levels = c(“sample”, “kriging”)) ggplot() + geom_sf(data = b2, mapping = aes(fill = Standard_error)) + facet_wrap(~var, as.table = FALSE) + scale_fill_gradientn(colors = sf.colors(20)) s = krige(NO2~1, no2.sf, grd, v.m, nmax = 30, nsim = 6) library(viridis) g = ggplot() + coord_equal() + scale_fill_viridis() + theme_void() + scale_x_discrete(expand=c(0,0)) + scale_y_discrete(expand=c(0,0)) g + geom_stars(data = s[,,,1:6]) + facet_wrap(~sample) v = vroom::vroom(“aq/pop/Zensus_Bevoelkerung_100m-Gitter.csv”) v |&gt; filter(Einwohner &gt; 0) |&gt; select(-Gitter_ID_100m) |&gt; st_as_sf(coords = c(“x_mp_100m”, “y_mp_100m”), crs = 3035) |&gt; st_transform(st_crs(grd)) -&gt; b a = aggregate(b, st_as_sf(grd, na.rm = FALSE), sum) v = vroom::vroom(“aq/pop/Zensus_Bevoelkerung_100m-Gitter.csv”) v1 &lt;- v[v\\(Einwohner &gt; 0,-1] rm(v); gc(full=TRUE) v2 &lt;- st_as_sf(v1, coords = c(&quot;x_mp_100m&quot;, &quot;y_mp_100m&quot;), crs = 3035) rm(v1); gc() b &lt;- st_transform(v2, st_crs(grd)) rm(v2); gc() a = aggregate(b, st_as_sf(grd, na.rm = FALSE), sum) gc() grd\\)ID = 1:prod(dim(grd)) # so we can find out later which grid cell we have ii = st_intersects(grd[“ID”], st_cast(st_union(de), “MULTILINESTRING”)) grd_sf = st_as_sf(grd[“ID”], na.rm = FALSE)[lengths(ii) &gt; 0,] iii = st_intersection(grd_sf, st_union(de)) grd\\(area = st_area(grd)[[1]] + units::set_units(grd\\)values, m^2) # NA’s grd\\(area[iii\\)ID] = st_area(iii) grd\\(pop_dens = a\\)Einwohner / grd\\(area sum(grd\\)pop_dens * grd\\(area, na.rm = TRUE) # verify sum(b\\)Einwohner) g + geom_stars(data = grd, aes(fill = sqrt(pop_dens), x = x, y = y)) (a = aggregate(grd[“pop_dens”], no2.sf, mean)) no2.sf\\(pop_dens = st_as_sf(a)[[1]] summary(lm(NO2~sqrt(pop_dens), no2.sf)) plot(NO2~sqrt(pop_dens), no2.sf) abline(lm(NO2~sqrt(pop_dens), no2.sf)) no2.sf = no2.sf[!is.na(no2.sf\\)pop_dens),] vr = variogram(NO2~sqrt(pop_dens), no2.sf) vr.m = fit.variogram(vr, vgm(1, “Exp”, 50000, 1)) plot(vr, vr.m, plot.numbers = TRUE) kr = krige(NO2~sqrt(pop_dens), no2.sf, grd[“pop_dens”], vr.m) k\\(kr1 = k\\)var1.pred k\\(kr2 = kr\\)var1.pred st_redimension(k[c(“kr1”, “kr2”)], along = list(what = c(“kriging”, “residual kriging”))) |&gt; setNames(“NO2”) -&gt; km g + geom_stars(data = km, aes(fill = NO2, x = x, y = y)) + geom_sf(data = st_cast(de, “MULTILINESTRING”)) + geom_sf(data = no2.sf) + facet_wrap(~what) + coord_sf(lims_method = “geometry_bbox”) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-5.html", "Chapter 10 for units: (?)", " Chapter 10 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) # try(load(”data/ch16.rda”)) set.seed(123) files = list.files(“aq”, pattern = “*.csv”, full.names = TRUE) r = lapply(files[-1], function(f) read.csv(f)) Sys.setenv(TZ = “UTC”) # make sure times are not interpreted in local time zone r = lapply(r, function(f) { f\\(t = as.POSIXct(f\\)DatetimeBegin) f[order(f\\(t), ] } ) r = r[sapply(r, nrow) &gt; 1000] names(r) = sapply(r, function(f) unique(f\\)AirQualityStationEoICode)) length(r) == length(unique(names(r))) library(xts) r = lapply(r, function(f) xts(f\\(Concentration, f\\)t)) aq = do.call(cbind, r) sel = apply(aq, 2, function(x) mean(is.na(x)) &lt; 0.25) aqsel = aq[, sel] library(tidyverse) read.csv(“aq/AirBase_v8_stations.csv”, sep = “) |&gt; as_tibble() |&gt; filter(country_iso_code ==”DE”, station_type_of_area == “rural”, type_of_station == “Background”) -&gt; a2 library(sf) a2.sf = st_as_sf(a2, coords = c(“station_longitude_deg”, “station_latitude_deg”), crs = ‘EPSG:4326’) sel = colnames(aqsel) %in% a2\\(station_european_code aqsel = aqsel[, sel] tb = tibble(NO2 = apply(aqsel, 2, mean, na.rm = TRUE), station_european_code = colnames(aqsel)) crs = &#39;EPSG:32632&#39; right_join(a2.sf, tb) |&gt; st_transform(crs) -&gt; no2.sf # load German boundaries data(air, package = &quot;spacetime&quot;) de &lt;- st_transform(st_as_sf(DE_NUTS1), crs) ggplot() + geom_sf(data = de) + geom_sf(data = no2.sf, mapping = aes(col = NO2)) # load German boundaries data(air, package = &quot;spacetime&quot;) de &lt;- st_transform(st_as_sf(DE_NUTS1), crs) library(gstat) demo(cokriging) demo(cosimulation) aqx = aq[ , colnames(aq) %in% a2\\)station_european_code] sfc = st_geometry(a2.sf)[match(colnames(aqx), a2.sf$station_european_code)] library(stars) st_as_stars(NO2 = as.matrix(aqx)) |&gt; st_set_dimensions(names = c(“time”, “station”)) |&gt; st_set_dimensions(“time”, index(aqx)) |&gt; st_set_dimensions(“station”, sfc) -&gt; no2.st v.st = variogramST(NO2~1, no2.st[,1:(24*31)], tlags = 0:48, cores = getOption(“mc.cores”, 2)) v1 = plot(v.st) v2 = plot(v.st, map = FALSE) print(v1, split = c(1,1,2,1), more = TRUE) print(v2, split = c(2,1,2,1), more = FALSE) # product-sum prodSumModel &lt;- vgmST(“productSum”, space = vgm(150, “Exp”, 200, 0), time = vgm(20, “Sph”, 40, 0), k = 2) StAni = estiStAni(v.st, c(0,200000)) (fitProdSumModel &lt;- fit.StVariogram(v.st, prodSumModel, fit.method = 7, stAni = StAni, method = “L-BFGS-B”, control = list(parscale = c(1,10,1,1,0.1,1,10)), lower = rep(0.0001, 7))) plot(v.st, fitProdSumModel, wireframe = FALSE, all = TRUE, scales = list(arrows=FALSE), zlim = c(0,150)) plot(v.st, model = fitProdSumModel, wireframe = TRUE, all = TRUE, scales = list(arrows = FALSE), zlim = c(0, 185)) pt = st_sample(de, 2) t = st_get_dimension_values(no2.st, 1) st_as_stars(list(pts = matrix(1, length(t), length(pt)))) |&gt; st_set_dimensions(names = c(“time”, “station”)) |&gt; st_set_dimensions(“time”, t) |&gt; st_set_dimensions(“station”, pt) -&gt; new_pt no2.st &lt;- st_transform(no2.st, crs) new_ts &lt;- krigeST(NO2~1, data = no2.st[“NO2”], newdata = new_pt, nmax = 50, stAni = StAni, modelList = fitProdSumModel, progress = FALSE) plot(as.xts(new_ts[2])) t4 = t[(1:4 - 0.5) * (32430)] d = dim(grd) st_as_stars(pts = array(1, c(d[1], d[2], time = length(t4)))) |&gt; st_set_dimensions(“time”, t4) |&gt; st_set_dimensions(“x”, st_get_dimension_values(grd, “x”)) |&gt; st_set_dimensions(“y”, st_get_dimension_values(grd, “y”)) |&gt; st_set_crs(crs) -&gt; grd.st new_int &lt;- krigeST(NO2~1, data = no2.st[“NO2”], newdata = grd.st, nmax = 200, stAni = StAni, modelList = fitProdSumModel, progress = FALSE) names(new_int)[2] = “NO2” g + geom_stars(data = new_int, aes(fill = NO2, x = x, y = y)) + facet_wrap(~as.Date(time)) + geom_sf(data = st_cast(de, “MULTILINESTRING”)) + geom_sf(data = no2.sf, col = ‘grey’, cex = .5) + coord_sf(lims_method = “geometry_bbox”) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-6.html", "Chapter 11 for units: (?)", " Chapter 11 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) knitr::opts_chunk\\(set(echo = TRUE, paged.print=FALSE) library(sf) data(pol_pres15, package = &quot;spDataLarge&quot;) pol_pres15 |&gt; subset(select = c(TERYT, name, types)) |&gt; head() library(tmap, warn.conflicts = FALSE) tm_shape(pol_pres15) + tm_fill(&quot;types&quot;) if (!all(st_is_valid(pol_pres15))) pol_pres15 &lt;- st_make_valid(pol_pres15) library(spdep) args(poly2nb) system.time(pol_pres15 |&gt; poly2nb(queen = TRUE) -&gt; nb_q) nb_q old_use_s2 &lt;- sf_use_s2() sf_use_s2(TRUE) (pol_pres15 |&gt; st_transform(&quot;OGC:CRS84&quot;) -&gt; pol_pres15_ll) |&gt; poly2nb(queen = TRUE) -&gt; nb_q_s2 all.equal(nb_q, nb_q_s2, check.attributes=FALSE) (nb_q |&gt; n.comp.nb())\\)nc library(Matrix, warn.conflicts = FALSE) library(spatialreg, warn.conflicts = FALSE) nb_q |&gt; nb2listw(style =”B”) |&gt; as(“CsparseMatrix”) -&gt; smat library(igraph, warn.conflicts = FALSE) (smat |&gt; graph.adjacency() -&gt; g1) |&gt; count_components() tf &lt;- tempfile(fileext = “.gal”) write.nb.gal(nb_q, tf) pol_pres15 |&gt; st_geometry() |&gt; st_centroid(of_largest_polygon = TRUE) -&gt; coords (coords |&gt; tri2nb() -&gt; nb_tri) nb_tri |&gt; nbdists(coords) |&gt; unlist() |&gt; summary() (nb_tri |&gt; n.comp.nb())\\(nc (nb_tri |&gt; soi.graph(coords) |&gt; graph2nb() -&gt; nb_soi) (nb_soi |&gt; n.comp.nb() -&gt; n_comp)\\)nc table(n_comp\\(comp.id) opar &lt;- par(mar = c(0,0,0,0)+0.5) pol_pres15 |&gt; st_geometry() |&gt; plot(border = &quot;grey&quot;, lwd = 0.5) nb_soi |&gt; plot(coords = coords, add = TRUE, points = FALSE, lwd = 0.5) nb_tri |&gt; diffnb(nb_soi) |&gt; plot(coords = coords, col = &quot;orange&quot;, add = TRUE, points = FALSE, lwd = 0.5) par(opar) coords |&gt; knearneigh(k = 1) |&gt; knn2nb() |&gt; nbdists(coords) |&gt; unlist() |&gt; summary() system.time(coords |&gt; dnearneigh(0, 18000) -&gt; nb_d18) system.time(coords |&gt; dnearneigh(0, 18000, use_kd_tree = FALSE) -&gt; nb_d18a) all.equal(nb_d18, nb_d18a, check.attributes = FALSE) nb_d18 (nb_d18 |&gt; n.comp.nb() -&gt; n_comp)\\)nc table(n_comp\\(comp.id) (coords |&gt; dnearneigh(0, 18300) -&gt; nb_d183) (nb_d183 |&gt; n.comp.nb())\\)nc (coords |&gt; dnearneigh(0, 16000) -&gt; nb_d16) ((coords |&gt; knearneigh(k = 6) -&gt; knn_k6) |&gt; knn2nb() -&gt; nb_k6) (knn_k6 |&gt; knn2nb(sym = TRUE) -&gt; nb_k6s) (nb_k6s |&gt; n.comp.nb())\\(nc old_use_s2 &lt;- sf_use_s2() sf_use_s2(TRUE) pol_pres15_ll |&gt; st_geometry() |&gt; st_centroid(of_largest_polygon = TRUE) -&gt; coords_ll (coords_ll |&gt; dnearneigh(0, 18.3, use_s2=TRUE, dwithin=TRUE) -&gt; nb_d183_ll) isTRUE(all.equal(nb_d183, nb_d183_ll, check.attributes = FALSE)) if (packageVersion(&quot;s2&quot;) &gt; &quot;1.0.7&quot;) (coords_ll |&gt; dnearneigh(0, 18.3) -&gt; nb_d183_llce) isTRUE(all.equal(nb_d183_llce, nb_d183_ll, check.attributes = FALSE)) (coords_ll |&gt; knearneigh(k = 6) |&gt; knn2nb() -&gt; nb_k6_ll) isTRUE(all.equal(nb_k6, nb_k6_ll, check.attributes = FALSE)) nb_q |&gt; nbdists(coords_ll) |&gt; unlist() |&gt; summary() nb_q |&gt; nbdists(coords) |&gt; unlist() |&gt; summary() sf_use_s2(old_use_s2) args(nb2listw) args(spweights.constants) (nb_q |&gt; nb2listw(style = &quot;B&quot;) -&gt; lw_q_B) |&gt; spweights.constants() |&gt; data.frame() |&gt; subset(select = c(n, S0, S1, S2)) (nb_q |&gt; nb2listw(style = &quot;W&quot;) -&gt; lw_q_W) |&gt; spweights.constants() |&gt; data.frame() |&gt; subset(select = c(n, S0, S1, S2)) nb_d183 |&gt; nbdists(coords) |&gt; lapply(function(x) 1/(x/1000)) -&gt; gwts (nb_d183 |&gt; nb2listw(glist=gwts, style=&quot;B&quot;) -&gt; lw_d183_idw_B) |&gt; spweights.constants() |&gt; data.frame() |&gt; subset(select=c(n, S0, S1, S2)) try(nb_d16 |&gt; nb2listw(style=&quot;B&quot;) -&gt; lw_d16_B) nb_d16 |&gt; nb2listw(style=&quot;B&quot;, zero.policy=TRUE) |&gt; spweights.constants(zero.policy=TRUE) |&gt; data.frame() |&gt; subset(select=c(n, S0, S1, S2)) nb_q (nb_q |&gt; nblag(2) -&gt; nb_q2)[[2]] nblag_cumul(nb_q2) union.nb(nb_q2[[2]], nb_q2[[1]]) diameter(g1) g1 |&gt; shortest.paths() -&gt; sps (sps |&gt; apply(2, max) -&gt; spmax) |&gt; max() mr &lt;- which.max(spmax) pol_pres15\\)name0[mr] pol_pres15\\(sps1 &lt;- sps[,mr] tm1 &lt;- tm_shape(pol_pres15) + tm_fill(&quot;sps1&quot;, title = &quot;Shortest path\\ncount&quot;) coords[mr] |&gt; st_distance(coords) |&gt; c() |&gt; (function(x) x/1000)() |&gt; units::set_units(NULL) -&gt; pol_pres15\\)dist_52 library(ggplot2) g1 &lt;- ggplot(pol_pres15, aes(x = sps1, y = dist_52)) + geom_point() + xlab(“Shortest path count”) + ylab(“km distance”) gridExtra::grid.arrange(tmap_grob(tm1), g1, nrow=1) glance_htest &lt;- function(ht) c(ht\\(estimate, &quot;Std deviate&quot; = unname(ht\\)statistic), “p.value” = unname(ht\\(p.value)) set.seed(1) (pol_pres15 |&gt; nrow() |&gt; rnorm() -&gt; x) |&gt; moran.test(lw_q_B, randomisation = FALSE, alternative = &quot;two.sided&quot;) |&gt; glance_htest() beta &lt;- 0.0015 coords |&gt; st_coordinates() |&gt; subset(select = 1, drop = TRUE) |&gt; (function(x) x/1000)() -&gt; t (x + beta * t -&gt; x_t) |&gt; moran.test(lw_q_B, randomisation = FALSE, alternative = &quot;two.sided&quot;) |&gt; glance_htest() lm(x_t ~ t) |&gt; lm.morantest(lw_q_B, alternative = &quot;two.sided&quot;) |&gt; glance_htest() args(joincount.test) (pol_pres15 |&gt; st_drop_geometry() |&gt; subset(select = types, drop = TRUE) -&gt; Types) |&gt; table() Types |&gt; joincount.multi(listw = lw_q_B) Types |&gt; joincount.multi(listw = lw_d183_idw_B) args(moran.test) pol_pres15 |&gt; st_drop_geometry() |&gt; subset(select = I_turnout, drop = TRUE) -&gt; I_turnout I_turnout |&gt; moran.test(listw = lw_q_B, randomisation = FALSE) |&gt; glance_htest() lm(I_turnout ~ 1, pol_pres15) |&gt; lm.morantest(listw = lw_q_B) |&gt; glance_htest() (I_turnout |&gt; moran.test(listw = lw_q_B) -&gt; mtr) |&gt; glance_htest() set.seed(1) I_turnout |&gt; moran.mc(listw = lw_q_B, nsim = 999, return_boot = TRUE) -&gt; mmc c(&quot;Permutation bootstrap&quot; = var(mmc\\)t), “Analytical randomisation” = unname(mtr\\(estimate[3])) I_turnout |&gt; moran.plot(listw = lw_q_W, labels = pol_pres15\\)TERYT, cex = 1, pch = “.”, xlab = “I round turnout”, ylab = “lagged turnout”) -&gt; infl_W pol_pres15\\(hat_value &lt;- infl_W\\)hat tm_shape(pol_pres15) + tm_fill(“hat_value”) args(localmoran) I_turnout |&gt; localmoran(listw = lw_q_W) -&gt; locm all.equal(sum(locm[,1])/Szero(lw_q_W), unname(moran.test(I_turnout, lw_q_W)\\(estimate[1])) pva &lt;- function(pv) cbind(&quot;none&quot; = pv, &quot;FDR&quot; = p.adjust(pv, &quot;fdr&quot;), &quot;BY&quot; = p.adjust(pv, &quot;BY&quot;), &quot;Bonferroni&quot; = p.adjust(pv, &quot;bonferroni&quot;)) locm |&gt; subset(select = &quot;Pr(z != E(Ii))&quot;, drop = TRUE) |&gt; pva() -&gt; pvsp f &lt;- function(x) sum(x &lt; 0.005) apply(pvsp, 2, f) library(parallel) invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(I_turnout |&gt; localmoran_perm(listw = lw_q_W, nsim = 9999, iseed = 1) -&gt; locm_p) locm_p |&gt; subset(select = &quot;Pr(z != E(Ii))&quot;, drop = TRUE) |&gt; pva() -&gt; pvsp apply(pvsp, 2, f) locm_p |&gt; subset(select = &quot;Pr(z != E(Ii)) Sim&quot;, drop = TRUE) |&gt; pva() -&gt; pvsp apply(pvsp, 2, f) pol_pres15\\)locm_pv &lt;- p.adjust(locm[, “Pr(z != E(Ii))”], “fdr”) pol_pres15\\(locm_std_pv &lt;- p.adjust(locm_p[, &quot;Pr(z != E(Ii))&quot;], &quot;fdr&quot;) pol_pres15\\)locm_p_pv &lt;- p.adjust(locm_p[, “Pr(z != E(Ii)) Sim”], “fdr”) tm_shape(pol_pres15) + tm_fill(c(“locm_pv”, “locm_std_pv”, “locm_p_pv”), breaks=c(0, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 0.75, 1), title = “Pseudo p-valuesMoran’s I”, palette=“-YlOrBr”) + tm_facets(free.scales = FALSE, ncol = 2) + tm_layout(panel.labels = c(“Analytical conditional”, “Permutation std. dev.”, “Permutation rank”)) quadr &lt;- attr(locm, “quadr”)\\(mean a &lt;- table(addNA(quadr)) pol_pres15\\)hs_an_q &lt;- pol_pres15\\(hs_ac_q &lt;- pol_pres15\\)hs_cp_q &lt;- quadr is.na(pol_pres15\\(hs_an_q) &lt;- pol_pres15\\)locm_pv &gt;= 0.005 b &lt;- table(addNA(pol_pres15\\(hs_an_q)) is.na(pol_pres15\\)hs_ac_q) &lt;- pol_pres15\\(locm_std_pv &gt;= 0.005 c &lt;- table(addNA(pol_pres15\\)hs_ac_q)) is.na(pol_pres15\\(hs_cp_q) &lt;- pol_pres15\\)locm_p_pv &gt;= 0.005 d &lt;- table(addNA(pol_pres15\\(hs_cp_q)) t(rbind(&quot;Moran plot quadrants&quot; = a, &quot;Analytical cond.&quot; = b, &quot;Permutation std. cond.&quot; = c, &quot;Permutation rank cond.&quot; = d)) pol_pres15\\)hs_an_q &lt;- droplevels(pol_pres15\\(hs_an_q) pol_pres15\\)hs_ac_q &lt;- droplevels(pol_pres15\\(hs_ac_q) pol_pres15\\)hs_cp_q &lt;- droplevels(pol_pres15\\(hs_cp_q) tm_shape(pol_pres15) + tm_fill(c(&quot;hs_an_q&quot;, &quot;hs_ac_q&quot;, &quot;hs_cp_q&quot;), colorNA=&quot;grey95&quot;, textNA=&quot;Not \\&quot;interesting\\&quot;&quot;, title=&quot;Turnout hotspot status\\nLocal Moran&#39;s I&quot;, palette=RColorBrewer::brewer.pal(4, &quot;Set3&quot;)[-c(2,3)]) + tm_facets(free.scales=FALSE, ncol=2) + tm_layout(panel.labels= c(&quot;Analytical conditional&quot;, &quot;Permutation std. cond.&quot;, &quot;Permutation rank cond.&quot;)) lm(I_turnout ~ 1) -&gt; lm_null invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(lm_null |&gt; localmoran.sad(nb = nb_q, style = &quot;W&quot;, alternative = &quot;two.sided&quot;) |&gt; summary() -&gt; locm_sad_null) lm(I_turnout ~ 1, weights = pol_pres15\\)I_entitled_to_vote) -&gt; lm_null_weights invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(lm_null_weights |&gt; localmoran.sad(nb = nb_q, style = “W”, alternative = “two.sided”) |&gt; summary() -&gt; locm_sad_null_weights) lm(I_turnout ~ Types, weights=pol_pres15\\(I_entitled_to_vote) -&gt; lm_types invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(lm_types |&gt; localmoran.sad(nb = nb_q, style = &quot;W&quot;, alternative = &quot;two.sided&quot;) |&gt; summary() -&gt; locm_sad_types) pol_pres15\\)locm_sad0 &lt;- pol_pres15\\(locm_sad1 &lt;- pol_pres15\\)locm_sad2 &lt;- quadr is.na(pol_pres15\\(locm_sad0) &lt;- p.adjust(locm_sad_null[, &quot;Pr. (Sad)&quot;], &quot;fdr&quot;) &gt;= 0.005 pol_pres15\\)locm_sad0 &lt;- droplevels(pol_pres15\\(locm_sad0) is.na(pol_pres15\\)locm_sad1) &lt;- p.adjust(locm_sad_null_weights[, “Pr. (Sad)”], “fdr”) &gt;= 0.005 pol_pres15\\(locm_sad1 &lt;- droplevels(pol_pres15\\)locm_sad1) is.na(pol_pres15\\(locm_sad2) &lt;- p.adjust(locm_sad_types[, &quot;Pr. (Sad)&quot;], &quot;fdr&quot;) &gt;= 0.005 pol_pres15\\)locm_sad2 &lt;- droplevels(pol_pres15\\(locm_sad2) tm_shape(pol_pres15) + tm_fill(c(&quot;hs_cp_q&quot;, &quot;locm_sad0&quot;, &quot;locm_sad1&quot;, &quot;locm_sad2&quot;), colorNA=&quot;grey95&quot;, textNA=&quot;Not \\&quot;interesting\\&quot;&quot;, title=&quot;Turnout hotspot status\\nLocal Moran&#39;s I&quot;, palette=RColorBrewer::brewer.pal(4, &quot;Set3&quot;)[c(1, 4, 2)]) + tm_facets(free.scales = FALSE, ncol = 2) + tm_layout(panel.labels = c( &quot;Permutation rank&quot;, &quot;Saddlepoint null&quot;, &quot;Saddlepoint weighted null&quot;, &quot;Saddlepoint weighted types&quot;)) rbind(null=append(table(addNA(pol_pres15\\)locm_sad0)), c(“Low-High”=0), 1), weighted=append(table(addNA(pol_pres15\\(locm_sad1)), c(&quot;Low-High&quot;=0), 1), type_weighted=table(addNA(pol_pres15\\)locm_sad2))) invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(lm_types |&gt; localmoran.exact(nb = nb_q, style = “W”, alternative = “two.sided”, useTP=TRUE, truncErr=1e-8) |&gt; as.data.frame() -&gt; locm_ex_types) pol_pres15\\(locm_ex &lt;- quadr is.na(pol_pres15\\)locm_ex) &lt;- p.adjust(locm_ex_types[, “Pr. (exact)”], “fdr”) &gt;= 0.005 pol_pres15\\(locm_ex &lt;- droplevels(pol_pres15\\)locm_ex) tm_shape(pol_pres15) + tm_fill(c(“locm_sad2”, “locm_ex”), colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“Turnout hotspot statusMoran’s I”, palette=RColorBrewer::brewer.pal(4, “Set3”)[c(1, 2, 4)]) + tm_facets(free.scales = FALSE, ncol = 2) + tm_layout(panel.labels = c( “Saddlepoint weighted types”, “Exact weighted types”)) table(Saddlepoint=addNA(pol_pres15\\(locm_sad2), exact=addNA(pol_pres15\\)locm_ex)) system.time(I_turnout |&gt; localG(lw_q_W, return_internals=TRUE) -&gt; locG) invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(I_turnout |&gt; localG_perm(lw_q_W, nsim = 9999, iseed = 1) -&gt; locG_p) cor(cbind(localG=attr(locG, “internals”)[, “Pr(z != E(Gi))”], attr(locG_p, “internals”)[, c(“Pr(z != E(Gi))”, “Pr(z != E(Gi)) Sim”)])) invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(I_turnout |&gt; localC_perm(lw_q_W, nsim=9999, iseed=1) -&gt; locC_p) cor(attr(locC_p, “pseudo-p”)[, c(“Pr(z != E(Ci))”, “Pr(z != E(Ci)) Sim”)]) pol_pres15\\(hs_C &lt;- attr(locC_p, &quot;cluster&quot;) is.na(pol_pres15\\)hs_C) &lt;- p.adjust(attr(locC_p, “pseudo-p”)[,“Pr(z != E(Ci)) Sim”], “fdr”) &gt;= 0.005 pol_pres15\\(hs_C &lt;- droplevels(pol_pres15\\)hs_C) pol_pres15\\(hs_G &lt;- cut(I_turnout, c(-Inf, mean(I_turnout), Inf), labels = c(&quot;Low&quot;, &quot;High&quot;)) is.na(pol_pres15\\)hs_G) &lt;- p.adjust(attr(locG_p, “internals”)[,“Pr(z != E(Gi))”], “fdr”) &gt;= 0.005 pol_pres15\\(hs_G &lt;- droplevels(pol_pres15\\)hs_G) m1 &lt;- tm_shape(pol_pres15) + tm_fill(“hs_cp_q”, palette=RColorBrewer::brewer.pal(4, “Set3”)[-c(2,3)], colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“Turnout hotspot statusMoran I”) m2 &lt;- tm_shape(pol_pres15) + tm_fill(“hs_G”, palette=RColorBrewer::brewer.pal(4, “Set3”)[-c(2,3)], colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“Turnout hotspot statusGetis/Ord G”) m3 &lt;- tm_shape(pol_pres15) + tm_fill(“hs_C”, palette=RColorBrewer::brewer.pal(4, “Set3”)[c(4, 1)], colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“Turnout hotspot statusGeary C”) tmap_arrange(m1, m2, m3, nrow=1) invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(pol_pres15 |&gt; st_drop_geometry() |&gt; subset(select = II_turnout) |&gt; localC_perm(lw_q_W, nsim=9999, iseed=1) -&gt; locC_p_II) pol_pres15\\(hs_C_II &lt;- attr(locC_p_II, &quot;cluster&quot;) is.na(pol_pres15\\)hs_C_II) &lt;- p.adjust(attr(locC_p_II, “pseudo-p”)[,“Pr(z != E(Ci)) Sim”], “fdr”) &gt;= 0.005 pol_pres15\\(hs_C_II &lt;- droplevels(pol_pres15\\)hs_C_II) invisible(spdep::set.coresOption(ifelse(detectCores() == 1, 1, detectCores()-1L))) system.time(pol_pres15 |&gt; st_drop_geometry() |&gt; subset(select = c(I_turnout, II_turnout)) |&gt; localC_perm(lw_q_W, nsim=9999, iseed=1) -&gt; locMvC_p) all.equal(locMvC_p, (locC_p+locC_p_II)/2, check.attributes=FALSE) pol_pres15\\(hs_MvC &lt;- attr(locMvC_p, &quot;cluster&quot;) is.na(pol_pres15\\)hs_MvC) &lt;- p.adjust(attr(locMvC_p, “pseudo-p”)[,“Pr(z != E(Ci)) Sim”], “fdr”) &gt;= 0.005 pol_pres15\\(hs_MvC &lt;- droplevels(pol_pres15\\)hs_MvC) m3 &lt;- tm_shape(pol_pres15) + tm_fill(“hs_C”, palette=RColorBrewer::brewer.pal(4, “Set3”)[c(4, 1)], colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“First round turnoutGeary C”) m4 &lt;- tm_shape(pol_pres15) + tm_fill(“hs_C_II”, palette=RColorBrewer::brewer.pal(4, “Set3”)[c(4, 1, 3, 2)], colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“Second round turnoutGeary C”) m5 &lt;- tm_shape(pol_pres15) + tm_fill(“hs_MvC”, palette=RColorBrewer::brewer.pal(4, “Set3”)[c(4, 1)], colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“Both rounds turnoutMultivariate Geary C”) tmap_arrange(m3, m4, m5, nrow=1) table(droplevels(interaction(addNA(pol_pres15\\(hs_C), addNA(pol_pres15\\)hs_C_II), sep=“:”)), addNA(pol_pres15\\(hs_MvC)) library(rgeoda) system.time(Geoda_w &lt;- queen_weights(pol_pres15)) summary(Geoda_w) system.time(lisa &lt;- local_multigeary(Geoda_w, pol_pres15[c(&quot;I_turnout&quot;, &quot;II_turnout&quot;)], cpu_threads = ifelse(parallel::detectCores() == 1, 1, parallel::detectCores()-1L), permutations = 99999, seed = 1)) all.equal(card(nb_q), lisa_num_nbrs(lisa), check.attributes = FALSE) all.equal(lisa_values(lisa), c(locMvC_p), check.attributes = FALSE) apply(attr(locMvC_p, &quot;pseudo-p&quot;)[,c(&quot;Pr(z != E(Ci)) Sim&quot;, &quot;Pr(folded) Sim&quot;)], 2, range) hs_MvCa &lt;- attr(locMvC_p, &quot;cluster&quot;) is.na(hs_MvCa) &lt;- p.adjust(attr(locMvC_p, &quot;pseudo-p&quot;)[,&quot;Pr(folded) Sim&quot;], &quot;fdr&quot;) &gt;= 0.0025 pol_pres15\\)hs_MvCa &lt;- droplevels(hs_MvCa) mvc &lt;- factor(lisa_clusters(lisa), levels=0:2, labels=lisa_labels(lisa)[1:3]) is.na(mvc) &lt;- p.adjust(lisa_pvalues(lisa), “fdr”) &gt;= 0.0025 pol_pres15\\(geoda_mvc &lt;- droplevels(mvc) addmargins(table(spdep=addNA(pol_pres15\\)hs_MvCa), rgeoda=addNA(pol_pres15$geoda_mvc))) m5 &lt;- tm_shape(pol_pres15) + tm_fill(“hs_MvCa”, palette=RColorBrewer::brewer.pal(4, “Set3”)[c(4, 1)], colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“Both rounds turnout (spdep)Multivariate Geary C”) m6 &lt;- tm_shape(pol_pres15) + tm_fill(“geoda_mvc”, palette=RColorBrewer::brewer.pal(4, “Set3”)[c(4, 1)], colorNA=“grey95”, textNA=“Not \"interesting\"”, title=“Both rounds turnout (rgeoda)Multivariate Geary C”) tmap_arrange(m5, m6, nrow=1) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-7.html", "Chapter 12 for units: (?)", " Chapter 12 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) knitr::opts_chunk\\(set(echo = TRUE, paged.print = FALSE) library(sf) library(spData) boston_506 &lt;- st_read(system.file(&quot;shapes/boston_tracts.shp&quot;, package = &quot;spData&quot;)[1]) nb_q &lt;- spdep::poly2nb(boston_506) lw_q &lt;- spdep::nb2listw(nb_q, style = &quot;W&quot;) table(boston_506\\)censored) summary(boston_506\\(median) boston_506\\)CHAS &lt;- as.factor(boston_506\\(CHAS) boston_489 &lt;- boston_506[!is.na(boston_506\\)median),] nb_q_489 &lt;- spdep::poly2nb(boston_489) lw_q_489 &lt;- spdep::nb2listw(nb_q_489, style =”W”, zero.policy = TRUE) agg_96 &lt;- list(as.character(boston_506\\(NOX_ID)) boston_96 &lt;- aggregate(boston_506[, &quot;NOX_ID&quot;], by = agg_96, unique) nb_q_96 &lt;- spdep::poly2nb(boston_96) lw_q_96 &lt;- spdep::nb2listw(nb_q_96) boston_96\\)NOX &lt;- aggregate(boston_506\\(NOX, agg_96, mean)\\)x boston_96\\(CHAS &lt;- aggregate(as.integer(boston_506\\)CHAS)-1, agg_96, max)\\(x nms &lt;- names(boston_506) ccounts &lt;- 23:31 for (nm in nms[c(22, ccounts, 36)]) { boston_96[[nm]] &lt;- aggregate(boston_506[[nm]], agg_96, sum)\\)x } br2 &lt;- c(3.50, 6.25, 8.75, 12.50, 17.50, 22.50, 30.00, 42.50, 60.00)*1000 counts &lt;- as.data.frame(boston_96)[, nms[ccounts]] f &lt;- function(x) matrixStats::weightedMedian(x = br2, w = x, interpolate = TRUE) boston_96\\(median &lt;- apply(counts, 1, f) is.na(boston_96\\)median) &lt;- boston_96\\(median &gt; 50000 summary(boston_96\\)median) POP &lt;- boston_506\\(POP f &lt;- function(x) matrixStats::weightedMean(x[,1], x[,2]) for (nm in nms[c(9:11, 14:19, 21, 33)]) { s0 &lt;- split(data.frame(boston_506[[nm]], POP), agg_96) boston_96[[nm]] &lt;- sapply(s0, f) } boston_94 &lt;- boston_96[!is.na(boston_96\\)median),] nb_q_94 &lt;- spdep::subset.nb(nb_q_96, !is.na(boston_96\\(median)) lw_q_94 &lt;- spdep::nb2listw(nb_q_94, style=&quot;W&quot;) boston_94a &lt;- aggregate(boston_489[,&quot;NOX_ID&quot;], list(boston_489\\)NOX_ID), unique) nb_q_94a &lt;- spdep::poly2nb(boston_94a) NOX_ID_no_neighs &lt;- boston_94a\\(NOX_ID[which(spdep::card(nb_q_94a) == 0)] boston_487 &lt;- boston_489[is.na(match(boston_489\\)NOX_ID, NOX_ID_no_neighs)),] boston_93 &lt;- aggregate(boston_487[, “NOX_ID”], list(ids = boston_487\\(NOX_ID), unique) row.names(boston_93) &lt;- as.character(boston_93\\)NOX_ID) nb_q_93 &lt;- spdep::poly2nb(boston_93, row.names=unique(as.character(boston_93\\(NOX_ID))) form &lt;- formula(log(median) ~ CRIM + ZN + INDUS + CHAS + I((NOX*10)^2) + I(RM^2) + AGE + log(DIS) + log(RAD) + TAX + PTRATIO + I(BB/100) + log(I(LSTAT/100))) library(Matrix) library(lme4) MLM &lt;- lmer(update(form, . ~ . + (1 | NOX_ID)), data = boston_487, REML = FALSE) boston_93\\)MLM_re &lt;- ranef(MLM)[[1]][,1] suppressPackageStartupMessages(library(hglm)) suppressWarnings(HGLM_iid &lt;- hglm(fixed = form, random = ~1 | NOX_ID, data = boston_487, family = gaussian())) boston_93\\(HGLM_re &lt;- unname(HGLM_iid\\)ranef) W &lt;- as(spdep::nb2listw(nb_q_93, style = “B”), “CsparseMatrix”) suppressWarnings(HGLM_car &lt;- hglm(fixed = form, random = ~ 1 | NOX_ID, data = boston_487, family = gaussian(), rand.family = CAR(D=W))) boston_93\\(HGLM_ss &lt;- HGLM_car\\)ranef[,1] library(HSAR) library(MatrixModels, warn.conflicts = FALSE) Z &lt;- as(model.Matrix(~ -1 + as.factor(NOX_ID), data = boston_487, sparse = TRUE), “dgCMatrix”) set.seed(123) suppressWarnings(HSAR_ss &lt;- hsar(form, data=boston_487, W = NULL, M = W, Delta = Z, burnin = 2000, Nsim = 12000, thinning = 2)) boston_93\\(HSAR_ss &lt;- HSAR_ss\\)Mus[1,] suppressPackageStartupMessages(library(R2BayesX)) BX_iid &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs = “re”)), family = “gaussian”, data = boston_487, method = “MCMC”, iterations = 12000, burnin = 2000, step = 2, seed = 123) boston_93\\(BX_re &lt;- BX_iid\\)effects[“sx(NOX_ID):re”][[1]]\\(Mean RBX_gra &lt;- nb2gra(nb_q_93) all.equal(row.names(RBX_gra), attr(nb_q_93, &quot;region.id&quot;)) all.equal(unname(diag(RBX_gra)), spdep::card(nb_q_93)) BX_mrf &lt;- bayesx(update(form, . ~ . + sx(NOX_ID, bs = &quot;mrf&quot;, map = RBX_gra)), family = &quot;gaussian&quot;, data = boston_487, method = &quot;MCMC&quot;, iterations = 12000, burnin = 2000, step = 2, seed = 123) boston_93\\)BX_ss &lt;- BX_mrf\\(effects[&quot;sx(NOX_ID):mrf&quot;][[1]]\\)Mean suppressPackageStartupMessages(library(INLA)) INLA_iid &lt;- inla(update(form, . ~ . + f(NOX_ID, model = “iid”)), family = “gaussian”, data = boston_487) boston_93\\(INLA_re &lt;- INLA_iid\\)summary.random\\(NOX_ID\\)mean ID2 &lt;- as.integer(as.factor(boston_487\\(NOX_ID)) INLA_ss &lt;- inla(update(form, . ~ . + f(ID2, model = &quot;besag&quot;, graph = W)), family = &quot;gaussian&quot;, data = boston_487) boston_93\\)INLA_ss &lt;- INLA_ss\\(summary.random\\)ID2\\(mean M &lt;- Diagonal(nrow(W), rowSums(W)) - W Cmatrix &lt;- Diagonal(nrow(M), 1) - M INLA_lr &lt;- inla(update(form, . ~ . + f(ID2, model = &quot;generic1&quot;, Cmatrix = Cmatrix)), family = &quot;gaussian&quot;, data = boston_487) boston_93\\)INLA_lr &lt;- INLA_lr\\(summary.random\\)ID2\\(mean library(mgcv) names(nb_q_93) &lt;- attr(nb_q_93, &quot;region.id&quot;) boston_487\\)NOX_ID &lt;- as.factor(boston_487\\(NOX_ID) GAM_MRF &lt;- gam(update(form, . ~ . + s(NOX_ID, bs = &quot;mrf&quot;, xt = list(nb = nb_q_93))), data = boston_487, method = &quot;REML&quot;) ssre &lt;- predict(GAM_MRF, type = &quot;terms&quot;, se = FALSE)[, &quot;s(NOX_ID)&quot;] all(sapply(tapply(ssre, list(boston_487\\)NOX_ID), c), function(x) length(unique(x)) == 1)) boston_93\\(GAM_ss &lt;- aggregate(ssre, list(boston_487\\)NOX_ID), head, n=1)\\(x library(tmap, warn.conflicts=FALSE) tm_shape(boston_93) + tm_fill(c(&quot;MLM_re&quot;, &quot;HGLM_re&quot;, &quot;INLA_re&quot;, &quot;BX_re&quot;), midpoint = 0, title = &quot;IID&quot;) + tm_facets(free.scales = FALSE) + tm_borders(lwd = 0.3, alpha = 0.4) + tm_layout(panel.labels = c(&quot;lmer&quot;, &quot;hglm&quot;, &quot;inla&quot;, &quot;bayesx&quot;)) tm_shape(boston_93) + tm_fill(c(&quot;HGLM_ss&quot;, &quot;HSAR_ss&quot;, &quot;INLA_lr&quot;, &quot;INLA_ss&quot;, &quot;BX_ss&quot;, &quot;GAM_ss&quot;), midpoint = 0, title = &quot;SSRE&quot;) + tm_facets(free.scales = FALSE) + tm_borders(lwd = 0.3, alpha = 0.4) + tm_layout(panel.labels = c(&quot;hglm CAR&quot;, &quot;hsar SAR&quot;, &quot;inla Leroux&quot;, &quot;inla ICAR&quot;, &quot;bayesx ICAR&quot;, &quot;gam ICAR&quot;)) library(spatialreg) eigs_489 &lt;- eigenw(lw_q_489) SDEM_489 &lt;- errorsarlm(form, data = boston_489, listw = lw_q_489, Durbin = TRUE, zero.policy = TRUE, control = list(pre_eig = eigs_489)) SEM_489 &lt;- errorsarlm(form, data = boston_489, listw = lw_q_489, zero.policy = TRUE, control = list(pre_eig = eigs_489)) cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(Hausman.test(SEM_489)), broom::tidy(Hausman.test(SDEM_489))))[,1:4] eigs_94 &lt;- eigenw(lw_q_94) SDEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, Durbin=TRUE, control=list(pre_eig=eigs_94)) SEM_94 &lt;- errorsarlm(form, data=boston_94, listw=lw_q_94, control=list(pre_eig=eigs_94)) cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(Hausman.test(SEM_94)), broom::tidy(Hausman.test(SDEM_94))))[, 1:4] cbind(data.frame(model=c(&quot;SEM&quot;, &quot;SDEM&quot;)), rbind(broom::tidy(LR1.Sarlm(SEM_94)), broom::tidy(LR1.Sarlm(SDEM_94))))[,c(1, 4:6)] broom::tidy(lmtest::lrtest(SEM_489, SDEM_489)) broom::tidy(lmtest::lrtest(SEM_94, SDEM_94)) SLX_489 &lt;- lmSLX(form, data = boston_489, listw = lw_q_489, zero.policy = TRUE) broom::tidy(lmtest::lrtest(SLX_489, SDEM_489)) SLX_94 &lt;- lmSLX(form, data = boston_94, listw = lw_q_94) broom::tidy(lmtest::lrtest(SLX_94, SDEM_94)) SLX_489w &lt;- lmSLX(form, data = boston_489, listw = lw_q_489, weights = units, zero.policy = TRUE) SDEM_489w &lt;- errorsarlm(form, data = boston_489, listw = lw_q_489, Durbin =TRUE, weights = units, zero.policy = TRUE, control = list(pre_eig = eigs_489)) broom::tidy(lmtest::lrtest(SLX_489w, SDEM_489w)) SLX_94w &lt;- lmSLX(form, data = boston_94, listw = lw_q_94, weights = units) SDEM_94w &lt;- errorsarlm(form, data = boston_94, listw = lw_q_94, Durbin = TRUE, weights = units, control = list(pre_eig = eigs_94)) broom::tidy(lmtest::lrtest(SLX_94w, SDEM_94w)) sum_imp_94_SDEM &lt;- summary(impacts(SDEM_94)) rbind(Impacts = sum_imp_94_SDEM\\)mat[5,], SE = sum_imp_94_SDEM\\(semat[5,]) sum_imp_94_SLX &lt;- summary(impacts(SLX_94)) rbind(Impacts = sum_imp_94_SLX\\)mat[5,], SE = sum_imp_94_SLX\\(semat[5,]) sum_imp_94_SDEMw &lt;- summary(impacts(SDEM_94w)) rbind(Impacts = sum_imp_94_SDEMw\\)mat[5,], SE = sum_imp_94_SDEMw\\(semat[5,]) sum_imp_94_SLXw &lt;- summary(impacts(SLX_94w)) rbind(Impacts = sum_imp_94_SLXw\\)mat[5,], SE = sum_imp_94_SLXw\\(semat[5,]) nd &lt;- boston_506[is.na(boston_506\\)median),] t0 &lt;- exp(predict(SDEM_489, newdata = nd, listw = lw_q, pred.type = “TS”, zero.policy =TRUE)) suppressWarnings(t1 &lt;- exp(predict(SDEM_489, newdata = nd, listw = lw_q, pred.type = “KP2”, zero.policy = TRUE))) suppressWarnings(t2 &lt;- exp(predict(SDEM_489, newdata = nd, listw = lw_q, pred.type = “KP5”, zero.policy = TRUE))) library(INLA) W &lt;- as(lw_q, “CsparseMatrix”) n &lt;- nrow(W) e &lt;- eigenw(lw_q) re.idx &lt;- which(abs(Im(e)) &lt; 1e-6) rho.max &lt;- 1/max(Re(e[re.idx])) rho.min &lt;- 1/min(Re(e[re.idx])) rho &lt;- mean(c(rho.min, rho.max)) boston_506\\(idx &lt;- 1:n zero.variance = list(prec=list(initial = 25, fixed = TRUE)) args.slm &lt;- list(rho.min = rho.min, rho.max = rho.max, W = W, X = matrix(0, n, 0), Q.beta = matrix(1,0,0)) hyper.slm &lt;- list(prec = list(prior = &quot;loggamma&quot;, param = c(0.01, 0.01)), rho = list(initial = 0, prior = &quot;logitbeta&quot;, param = c(1,1))) WX &lt;- create_WX(model.matrix(update(form, CMEDV ~ .), data=boston_506), lw_q) SDEM_506_slm &lt;- inla(update(form, . ~ . + WX + f(idx, model = &quot;slm&quot;, args.slm = args.slm, hyper = hyper.slm)), data = boston_506, family = &quot;gaussian&quot;, control.family = list(hyper = zero.variance), control.compute = list(dic = TRUE, cpo = TRUE)) mvs &lt;- SDEM_506_slm\\)marginals.fitted.values[which(is.na(boston_506\\(median))] mv_mean &lt;- sapply(mvs, function(x) mean(exp(x[, 1]), )) mv_sd &lt;- sapply(mvs, function(x) sd(exp(x[, 1]))) data.frame(fit_TS = t0[,1], fit_KP2 = c(t1), fit_KP5 = c(t2), INLA_slm = mv_mean, INLA_slm_sd = mv_sd, censored = boston_506\\)censored[as.integer(attr(t0, “region.id”))]) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-8.html", "Chapter 13 for units: (?)", " Chapter 13 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) library(sp) y = as(x,”Spatial”) x0 = st_as_sf(y) x = as(sf::read_sf(“file”), “Spatial”) set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-9.html", "Chapter 14 for units: (?)", " Chapter 14 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) # All R code in this book {-} set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-10.html", "Chapter 15 for units: (?)", " Chapter 15 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) a |&gt; b() |&gt; c() |&gt; d(n = 10) d(c(b(a)), n = 10) tmp1 &lt;- b(a) tmp2 &lt;- c(tmp1) tmp3 &lt;- d(tmp2, n = 10) typeof(1:10) length(1:10) typeof(1.0) length(1.0) typeof(c(”foo”, “bar”)) length(c(“foo”, “bar”)) typeof(c(TRUE, FALSE)) i = integer(0) typeof(i) i length(i) a = c(1,2,3) a[2] a[[2]] a[2:3] a[2:3] = c(5,6) a a[[3]] = 10 a l &lt;- list(3, TRUE, “foo”) typeof(l) length(l) l[1] l[[1]] l[1:2] = list(4, FALSE) l l[[3]] = “bar” l l = list(first = 3, second = TRUE, third = “foo”) l l\\(second l\\)second = FALSE l 3 == NULL # not FALSE! NULL == NULL # not even TRUE! is.null(NULL) l = l[c(1,3)] # remove second, implicitly l l\\(second = NULL l a = 1:3 attr(a, &quot;some_meta_data&quot;) = &quot;foo&quot; a attr(a, &quot;some_meta_data&quot;) attr(a, &quot;some_meta_data&quot;) = &quot;bar&quot; attr(a, &quot;some_meta_data&quot;) attributes(a) attributes(a) = list(some_meta_data = &quot;foo&quot;) attributes(a) class(1:3) class(c(TRUE, FALSE)) class(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) a = 1:3 class(a) = &quot;foo&quot; a class(a) attributes(a) print.foo = function(x, ...) print(paste(&quot;an object of class foo with length&quot;, length(x))) print(a) unclass(a) library(sf) p = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,0)))) p unclass(p) a = 1:8 class(a) attr(a, &quot;dim&quot;) = c(2,4) # or: dim(a) = c(2,4) class(a) a attr(a, &quot;dim&quot;) = c(2,2,2) # or: dim(a) = c(2,2,2) class(a) a a = c(first = 3, second = 4, last = 5) a[&quot;second&quot;] attributes(a) a = matrix(1:4, 2, 2) dimnames(a) = list(rows = c(&quot;row1&quot;, &quot;row2&quot;), cols = c(&quot;col1&quot;, &quot;col2&quot;)) a attributes(a) df = data.frame(a = 1:3, b = c(TRUE, FALSE, TRUE)) attributes(df) f = function(x) { a = create_obj(x) # call some other function attributes(a) = list(class = &quot;foo&quot;, meta = 33) a } f = function(x) { a = create_obj(x) # call some other function structure(a, class = &quot;foo&quot;, meta = 33) } library(sf) system.file(&quot;gpkg/nc.gpkg&quot;, package = &quot;sf&quot;) %&gt;% read_sf() -&gt; nc attributes(nc) nc\\)geom attributes(nc\\(geom) nc\\)geom[[4]] typeof(nc\\(geom[[4]]) attributes(nc\\)geom[[4]]) length(nc\\(geom[[4]]) lengths(nc\\)geom) typeof(nc\\(geom[[4]]) typeof(nc\\)geom[[4]][[1]]) length(nc\\(geom[[4]][[1]]) typeof(nc\\)geom[[4]][[1]][[1]]) dim(nc\\(geom[[4]][[1]][[1]]) head(nc\\)geom[[4]][[1]][[1]]) nc$geom[[4]][[1]][[1]][3,2] = 36.5 set.seed(1014) options(digits = 3) knitr::opts_chunk$set( comment = “#”, collapse = knitr::is_latex_output(), cache = TRUE, # out.width = “70%”, fig.align = ‘center’ # fig.width = 6, # fig.asp = 0.618, # 1 / phi # fig.show = “hold” ) options(dplyr.print_min = 6, dplyr.print_max = 6) options(stars.crs = 17) mapview::mapviewOptions(fgb = FALSE) "],["for-units-11.html", "Chapter 16 for units: (?) Data structures dissecting a MULTIPOLYGON", " Chapter 16 for units: (?) Sys.setenv(UDUNITS2_XML_PATH=““) if (knitr::is_latex_output()) options(width = 66) #options(width = 72) &lt;!--chapter:end:97-allcode.Rmd--&gt; # R basics {-} This chapter provides some minimal set of R basics that may make it easier to read this book. A more comprehensive book on R basics is given in [@advr], chapter 2. ## Pipes {-} The `|&gt;` (pipe) symbols should be read as _then_: we read ```r a |&gt; b() |&gt; c() |&gt; d(n = 10) as with a do b then c then d with n being 10, and that is just alternative syntax for d(c(b(a)), n = 10) or tmp1 &lt;- b(a) tmp2 &lt;- c(tmp1) tmp3 &lt;- d(tmp2, n = 10) To many, the pipe-form is easier to read because execution order follows reading order, from left to right. Like nested function calls, it avoids the need to choose names for intermediate results, but like nested function calls, it is hard to debug intermediate results that diverge from out expectations. Note that the intermediate results do exist in memory, so neither form saves memory allocation. The pipe that appeared natively in R 4.1.0, |&gt;, can be used anywhere in this book where %&gt;% is used. The reason we kept to the %&gt;% pipe is to not exclude users of older R version to use the code sections in this book. R versions before 4.1 do not have a native pipe operator. For these, the |&gt; needs to be replaced by %&gt;%, another pipe operator provided by package magrittr. For all occasions used in this book, the two behave syntactically identical. Data structures As pointed out by (Chambers 2016), everything that exists in R is an object. This includes objects that make things happen, such as language objects or functions, but also the more basic “things”, such as data objects. Some basic R data structures will now be discussed. Homogeneous vectors Data objects contain data, and possibly metadata. Data is always in the form of a vector, which can have different type. We can find the type by typeof, and vector length by length. Vectors are created by c, which combines individual elements: typeof(1:10) # [1] &quot;integer&quot; length(1:10) # [1] 10 typeof(1.0) # [1] &quot;double&quot; length(1.0) # [1] 1 typeof(c(&quot;foo&quot;, &quot;bar&quot;)) # [1] &quot;character&quot; length(c(&quot;foo&quot;, &quot;bar&quot;)) # [1] 2 typeof(c(TRUE, FALSE)) # [1] &quot;logical&quot; Vectors of this kind can only have a single type. Note that vectors can have zero length, e.g. in, i = integer(0) typeof(i) # [1] &quot;integer&quot; i # integer(0) length(i) # [1] 0 We can retrieve (or in assignments: replace) elements in a vector using [ or [[: a = c(1,2,3) a[2] # [1] 2 a[[2]] # [1] 2 a[2:3] # [1] 2 3 a[2:3] = c(5,6) a # [1] 1 5 6 a[[3]] = 10 a # [1] 1 5 10 where the difference is that [ can operate on an index range (or multiple indexes), and [[ operates on a single vector value. Heterogeneous vectors: list An additional vector type is the list, which can combine any types in its elements: l &lt;- list(3, TRUE, &quot;foo&quot;) typeof(l) # [1] &quot;list&quot; length(l) # [1] 3 For lists, there is a further distinction between [ and [[: the single [ returns always a list, and [[ returns the contents of a list element: l[1] # [[1]] # [1] 3 l[[1]] # [1] 3 For replacement, one case use [ when providing a list, and [[ when providing a new value: l[1:2] = list(4, FALSE) l # [[1]] # [1] 4 # # [[2]] # [1] FALSE # # [[3]] # [1] &quot;foo&quot; l[[3]] = &quot;bar&quot; l # [[1]] # [1] 4 # # [[2]] # [1] FALSE # # [[3]] # [1] &quot;bar&quot; In case list elements are named, as in l = list(first = 3, second = TRUE, third = &quot;foo&quot;) l # $first # [1] 3 # # $second # [1] TRUE # # $third # [1] &quot;foo&quot; we can use names as in l[[\"second\"]] and this can be abbreviated to l$second # [1] TRUE l$second = FALSE l # $first # [1] 3 # # $second # [1] FALSE # # $third # [1] &quot;foo&quot; This is convenient, but also requires name look-up in the names attribute (see below). NULL and removing list elements NULL is the null value in R; it is special in the sense that it doesn’t work in simple comparisons: 3 == NULL # not FALSE! # logical(0) NULL == NULL # not even TRUE! # logical(0) but has to be treated specially, using is.null: is.null(NULL) # [1] TRUE When we want to remove one or more list elements, we can do so by creating a new list that does not contain the elements that needed removal, as in l = l[c(1,3)] # remove second, implicitly l # $first # [1] 3 # # $third # [1] &quot;foo&quot; but we can also assign NULL to the element we want to eliminate: l$second = NULL l # $first # [1] 3 # # $third # [1] &quot;foo&quot; Attributes We can glue arbitrary metadata objects to data objects, as in a = 1:3 attr(a, &quot;some_meta_data&quot;) = &quot;foo&quot; a # [1] 1 2 3 # attr(,&quot;some_meta_data&quot;) # [1] &quot;foo&quot; and this can be retrieved, or replaced by attr(a, &quot;some_meta_data&quot;) # [1] &quot;foo&quot; attr(a, &quot;some_meta_data&quot;) = &quot;bar&quot; attr(a, &quot;some_meta_data&quot;) # [1] &quot;bar&quot; In essence, the attribute of an object is a named list, and we can get or set the complete list by attributes(a) # $some_meta_data # [1] &quot;bar&quot; attributes(a) = list(some_meta_data = &quot;foo&quot;) attributes(a) # $some_meta_data # [1] &quot;foo&quot; A number of attributes are treated specially by R, see e.g. ?attributes. object class and class attribute Every object in R “has a class”, meaning that class(obj) returns a character vector with the class of obj. Some objects have an implicit class, e.g. vectors class(1:3) # [1] &quot;integer&quot; class(c(TRUE, FALSE)) # [1] &quot;logical&quot; class(c(&quot;TRUE&quot;, &quot;FALSE&quot;)) # [1] &quot;character&quot; but we can also set the class explicit, either by using attr or by using class in the left-hand side of an expression: a = 1:3 class(a) = &quot;foo&quot; a # [1] 1 2 3 # attr(,&quot;class&quot;) # [1] &quot;foo&quot; class(a) # [1] &quot;foo&quot; attributes(a) # $class # [1] &quot;foo&quot; in which case the newly set class overrides the earlier implicit class. This way, we can add methods for class foo, e.g. by print.foo = function(x, ...) print(paste(&quot;an object of class foo with length&quot;, length(x))) print(a) # [1] &quot;an object of class foo with length 3&quot; Providing such methods are generally intended to create more usable software, but at the same time they may make the objects more opaque. It is sometimes useful to see what an object “is made of” by printing it after the class attribute is removed, as in unclass(a) # [1] 1 2 3 As a more elaborate example, consider the case where a polygon is made using package sf: library(sf) p = st_polygon(list(rbind(c(0,0), c(1,0), c(1,1), c(0,0)))) p # POLYGON ((0 0, 1 0, 1 1, 0 0)) which prints the well-known-text form; to understand what the data structure is like, we can use unclass(p) # [[1]] # [,1] [,2] # [1,] 0 0 # [2,] 1 0 # [3,] 1 1 # [4,] 0 0 the dim attribute The dim attribute sets the matrix or array dimensions: a = 1:8 class(a) # [1] &quot;integer&quot; attr(a, &quot;dim&quot;) = c(2,4) # or: dim(a) = c(2,4) class(a) # [1] &quot;matrix&quot; &quot;array&quot; a # [,1] [,2] [,3] [,4] # [1,] 1 3 5 7 # [2,] 2 4 6 8 attr(a, &quot;dim&quot;) = c(2,2,2) # or: dim(a) = c(2,2,2) class(a) # [1] &quot;array&quot; a # , , 1 # # [,1] [,2] # [1,] 1 3 # [2,] 2 4 # # , , 2 # # [,1] [,2] # [1,] 5 7 # [2,] 6 8 various names attributes Named vectors carry their names in a names attribute. We saw examples for lists above, an example for a numeric vector is: a = c(first = 3, second = 4, last = 5) a[&quot;second&quot;] # second # 4 attributes(a) # $names # [1] &quot;first&quot; &quot;second&quot; &quot;last&quot; More name attributes are e.g. dimnames of matrices or arrays, which not only names dimensions, but also the labels associated with each of the dimensions: a = matrix(1:4, 2, 2) dimnames(a) = list(rows = c(&quot;row1&quot;, &quot;row2&quot;), cols = c(&quot;col1&quot;, &quot;col2&quot;)) a # cols # rows col1 col2 # row1 1 3 # row2 2 4 attributes(a) # $dim # [1] 2 2 # # $dimnames # $dimnames$rows # [1] &quot;row1&quot; &quot;row2&quot; # # $dimnames$cols # [1] &quot;col1&quot; &quot;col2&quot; Data.frame objects have rows and columns, and each have names: df = data.frame(a = 1:3, b = c(TRUE, FALSE, TRUE)) attributes(df) # $names # [1] &quot;a&quot; &quot;b&quot; # # $class # [1] &quot;data.frame&quot; # # $row.names # [1] 1 2 3 using structure When programming, the pattern of adding or modifying attributes before returning an object is extremely common, an example being: f = function(x) { a = create_obj(x) # call some other function attributes(a) = list(class = &quot;foo&quot;, meta = 33) a } The last two statements can be contracted in f = function(x) { a = create_obj(x) # call some other function structure(a, class = &quot;foo&quot;, meta = 33) } where function structure adds, replaces, or (in case of value NULL) removes attributes from the object in its first argument. dissecting a MULTIPOLYGON We can use the above examples to dissect an sf object with MULTIPOLYGONs into pieces. Suppose we use the nc dataset, library(sf) system.file(&quot;gpkg/nc.gpkg&quot;, package = &quot;sf&quot;) %&gt;% read_sf() -&gt; nc we can see from the attributes of nc, attributes(nc) # $names # [1] &quot;AREA&quot; &quot;PERIMETER&quot; &quot;CNTY_&quot; &quot;CNTY_ID&quot; &quot;NAME&quot; &quot;FIPS&quot; # [7] &quot;FIPSNO&quot; &quot;CRESS_ID&quot; &quot;BIR74&quot; &quot;SID74&quot; &quot;NWBIR74&quot; &quot;BIR79&quot; # [13] &quot;SID79&quot; &quot;NWBIR79&quot; &quot;geom&quot; # # $row.names # [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 # [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 # [91] 91 92 93 94 95 96 97 98 99 100 # # $class # [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # # $sf_column # [1] &quot;geom&quot; # # $agr # AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID # &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; # BIR74 SID74 NWBIR74 BIR79 SID79 NWBIR79 # &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; # Levels: constant aggregate identity that the geometry column is named geom. When we take out this column, nc$geom # Geometry set for 100 features # Geometry type: MULTIPOLYGON # Dimension: XY # Bounding box: xmin: -84.3 ymin: 33.9 xmax: -75.5 ymax: 36.6 # Geodetic CRS: NAD27 # First 5 geometries: # MULTIPOLYGON (((-81.5 36.2, -81.5 36.3, -81.6 3... # MULTIPOLYGON (((-81.2 36.4, -81.2 36.4, -81.3 3... # MULTIPOLYGON (((-80.5 36.2, -80.5 36.3, -80.5 3... # MULTIPOLYGON (((-76 36.3, -76 36.3, -76 36.3, -... # MULTIPOLYGON (((-77.2 36.2, -77.2 36.2, -77.3 3... we see an object that has the following attributes attributes(nc$geom) # $n_empty # [1] 0 # # $crs # Coordinate Reference System: # User input: NAD27 # wkt: # GEOGCRS[&quot;NAD27&quot;, # DATUM[&quot;North American Datum 1927&quot;, # ELLIPSOID[&quot;Clarke 1866&quot;,6378206.4,294.978698213898, # LENGTHUNIT[&quot;metre&quot;,1]]], # PRIMEM[&quot;Greenwich&quot;,0, # ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], # CS[ellipsoidal,2], # AXIS[&quot;geodetic latitude (Lat)&quot;,north, # ORDER[1], # ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], # AXIS[&quot;geodetic longitude (Lon)&quot;,east, # ORDER[2], # ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], # USAGE[ # SCOPE[&quot;Geodesy.&quot;], # AREA[&quot;North and central America: Antigua and Barbuda - onshore. Bahamas - onshore plus offshore over internal continental shelf only. Belize - onshore. British Virgin Islands - onshore. Canada onshore - Alberta, British Columbia, Manitoba, New Brunswick, Newfoundland and Labrador, Northwest Territories, Nova Scotia, Nunavut, Ontario, Prince Edward Island, Quebec, Saskatchewan and Yukon - plus offshore east coast. Cuba - onshore and offshore. El Salvador - onshore. Guatemala - onshore. Honduras - onshore. Panama - onshore. Puerto Rico - onshore. Mexico - onshore plus offshore east coast. Nicaragua - onshore. United States (USA) onshore and offshore - Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin and Wyoming - plus offshore . US Virgin Islands - onshore.&quot;], # BBOX[7.15,167.65,83.17,-47.74]], # ID[&quot;EPSG&quot;,4267]] # # $class # [1] &quot;sfc_MULTIPOLYGON&quot; &quot;sfc&quot; # # $precision # [1] 0 # # $bbox # xmin ymin xmax ymax # -84.3 33.9 -75.5 36.6 When we take the contents of the fourth list element, we obtain nc$geom[[4]] # MULTIPOLYGON (((-76 36.3, -76 36.3, -76 36.3, -76 36.4, -76.1 36.3, -76.2 36.4, -76.2 36.4, -76.2 36.4, -76.3 36.6, -76.1 36.6, -76 36.6, -76 36.5, -76.1 36.5, -76 36.4, -76 36.4, -76 36.4, -76 36.4, -75.9 36.4, -75.9 36.4, -75.8 36.1, -75.8 36.1, -75.9 36.1, -75.9 36.2, -76 36.3, -75.9 36.3, -76 36.3)), ((-76 36.6, -76 36.6, -75.9 36.5, -75.9 36.5, -76 36.5, -76 36.5, -76 36.6)), ((-75.9 36.6, -75.9 36.6, -75.8 36.2, -75.8 36.2, -75.9 36.6))) which is a list, typeof(nc$geom[[4]]) # [1] &quot;list&quot; with attributes attributes(nc$geom[[4]]) # $class # [1] &quot;XY&quot; &quot;MULTIPOLYGON&quot; &quot;sfg&quot; and length length(nc$geom[[4]]) # [1] 3 The length indicates the number of outer rings: a multi-polygon can consist of more than one polygon. We see that most counties only have a single polygon: lengths(nc$geom) # [1] 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # [75] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 2 1 1 1 1 1 A multi-polygon is a list with polygons, typeof(nc$geom[[4]]) # [1] &quot;list&quot; and the first polygon of the fourth multi-polygon is again a list, because polygons have an outer ring possibly followed by multiple inner rings (holes) typeof(nc$geom[[4]][[1]]) # [1] &quot;list&quot; we see that it contains only one ring, the exterior ring: length(nc$geom[[4]][[1]]) # [1] 1 and we can print type, the dimension and the first set of coordinates by typeof(nc$geom[[4]][[1]][[1]]) # [1] &quot;double&quot; dim(nc$geom[[4]][[1]][[1]]) # [1] 26 2 head(nc$geom[[4]][[1]][[1]]) # [,1] [,2] # [1,] -76.0 36.3 # [2,] -76.0 36.3 # [3,] -76.0 36.3 # [4,] -76.0 36.4 # [5,] -76.1 36.3 # [6,] -76.2 36.4 and we can now for instance change the latitude of the third coordinate by nc$geom[[4]][[1]][[1]][3,2] = 36.5 References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
